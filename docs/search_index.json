[
["index.html", "useR! Machine Learning Tutorial Preface Overview Dimensionality Issues Sparsity Normalization Categorical Data Missing Data Class Imbalance Overfitting Software Scalability Resources", " useR! Machine Learning Tutorial Erin LeDell 2018-04-05 Preface Figure .: UseR 2016 useR! 2016 Tutorial: Machine Learning Algorithmic Deep Dive Overview This tutorial contains training modules for six popular supervised machine learning methods: Classification and Regression Trees (CART) Random Forests (RF)](random-forest) Gradient Boosting Machines (GBM) Generalized Linear Models (GLM) Deep Neural Networks (DNN) Stacking / Super Learner (SL) Here are some practical, related topics we will cover for each algorithm: Dimensionality Issues Sparsity Normalization Categorical Data Missing Data Class Imbalance Overfitting Software Scalability Instructions for how to install the necessary software for this tutorial is available here. Data for the tutorial can be downloaded… Dimensionality Issues Certain algorithms don’t scale well when there are millions of features. For example, decision trees require computing some sort of metric (to determine the splits) on all the feature values (or some fraction of the values as in Random Forest and Stochastic GBM). Therefore, computation time is linear in the number of features. Other algorithms, such as GLM, scale much better to high-dimensional (n &lt;&lt; p) and wide data with appropriate regularization (e.g. Lasso, Elastic Net, Ridge). Sparsity Algorithms can deal with data sparsity (where many of the feature values are zero) in different ways. In some algorithms there are ways to speed up the computations if sparsity is present, so it’s good to know if these shortcuts are available. Normalization Some algorithms such as Deep Neural Nets and GLMs require that data be normalized for effective interpretation of the models. Tree-based algorithms (Decision Trees, Random Forest, Gradient Boosting Machines) do not require normalization. Tree-based methods only use information about whether a value is greater than or less than a certain value (e.g. x &gt; 7 vs. x ≤ 7), the values themselves do not matter. Categorical Data Algorithms handle categorical data differently. Some algorithms such as GLM and Deep Neural Nets require that a categorical variable be expanded into a set of indicator variables, prior to training. With tree-based methods and software that supports it, there are ways to get around this requirement, which allows the algorithm to handle the categorical features directly. It is important to be cognizant of the cardinality of your categorical features before training, as additional pre-processing (collapsing categories, etc) may be beneficial with high-cardinality features. Missing Data Assuming the features are missing completely at random, there are a number of ways of handling missing data: Discard observations with any missing values. Rely on the learning algorithm to deal with missing values in its training phase. Impute all missing values before training. For most learning methods, the imputation approach (3) is necessary. The simplest tactic is to impute the missing value with the mean or median of the nonmissing values for that feature. If the features have at least some moderate degree of dependence, one can do better by estimating a predictive model for each feature given the other features and then imputing each missing value by its prediction from the model. Some software packages handle missing data automatically, although many don’t, so it’s important to be aware if any pre-processing is required by the user. Class Imbalance Algorithms that optimize a metric such as accuracy may fail to perform well on training sets that contain a significant degree of class imbalance. Certain algorithms, such as GBM, allow the user to optimize a performance metric of choice, which is useful when you have a highly imbalanced training set. Overfitting It is always good to pay attention to the potential of overfitting, but certain algorithms and certain implementations are more prone to this issue. For example, when using Deep Neural Nets and Gradient Boosting Machines, it’s always a good idea to check for overfitting. Software For each algorithm, we will provide examples of open source R packages that implement the algorithm. All implementations are different, so we will provide information on how each of the implementations differ. Scalability We will address scalability issues inherent to the algorithm and discuss algorithmic or technological solutions to scalability concerns for “big data.” Resources Where to learn more? An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani The Elements of Statistical Learning by Trevor Hastie, Rob Tibshirani and Jerome Friedman Practical Data Science with R by John Mount and Nina Zumel Applied Predictive Modeling by Max Kuhn and Kjell Johnson 15 hours of expert videos by Trevor Hastie and Rob Tibshirani "],
["decision-trees.html", "Chapter 1 Classification and Regression Trees (CART) 1.1 Introduction 1.2 Properties of Trees 1.3 Tree Algorithms 1.4 CART vs C4.5 1.5 Splitting Criterion &amp; Best Split 1.6 Decision Boundary 1.7 Missing Data 1.8 Visualizing Decision Trees 1.9 CART Software in R", " Chapter 1 Classification and Regression Trees (CART) Figure .: Decision Tree visualization by Tony Chu and Stephanie Yee. 1.1 Introduction Classification and regression trees (CART) are a non-parametric decision tree learning technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively. CART is both a generic term to describe tree algorithms and also a specific name for Breiman’s original algorithm for constructing classification and regression trees. Decision Tree: A tree-shaped graph or model of decisions used to determine a course of action or show a statistical probability. Classification Tree: A decision tree that performs classification (predicts a categorical response). Regression Tree: A decision tree that performs regression (predicts a numeric response). Split Point: A split point occurs at each node of the tree where a decision is made (e.g. x &gt; 7 vs. x ≤ 7). Terminal Node: A terminal node is a node which has no descendants (child nodes). Also called a “leaf node.” 1.2 Properties of Trees Can handle huge datasets. Can handle mixed predictors implicitly – numeric and categorical. Easily ignore redundant variables. Handle missing data elegantly through surrogate splits. Small trees are easy to interpret. Large trees are hard to interpret. Prediction performance is often poor (high variance). 1.3 Tree Algorithms There are a handful of different tree algorithms in addition to Breiman’s original CART algorithm. Namely, ID3, C4.5 and C5.0, all created by Ross Quinlan. C5.0 is an improvement over C4.5, however, the C4.5 algorithm is still quite popular since the multi-threaded version of C5.0 is proprietary (although the single threaded is released as GPL). 1.4 CART vs C4.5 Here are some of the differences between CART and C4.5: Tests in CART are always binary, but C4.5 allows two or more outcomes. CART uses the Gini diversity index to rank tests, whereas C4.5 uses information-based criteria. CART prunes trees using a cost-complexity model whose parameters are estimated by cross-validation; C4.5 uses a single-pass algorithm derived from binomial confidence limits. With respect to missing data, CART looks for surrogate tests that approximate the outcomes when the tested attribute has an unknown value, but C4.5 apportions the case probabilistically among the outcomes. Decision trees are formed by a collection of rules based on variables in the modeling data set: Rules based on variables’ values are selected to get the best split to differentiate observations based on the dependent variable. Once a rule is selected and splits a node into two, the same process is applied to each “child” node (i.e. it is a recursive procedure). Splitting stops when CART detects no further gain can be made, or some pre-set stopping rules are met. (Alternatively, the data are split as much as possible and then the tree is later pruned.) Each branch of the tree ends in a terminal node. Each observation falls into one and exactly one terminal node, and each terminal node is uniquely defined by a set of rules. 1.5 Splitting Criterion &amp; Best Split The original CART algorithm uses the Gini Impurity, whereas ID3, C4.5 and C5.0 use Entropy or Information Gain (related to Entropy). 1.5.1 Gini Impurity Used by the CART algorithm, Gini Impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. Gini impurity can be computed by summing the probability \\(f_i\\) of each item being chosen times the probability \\(1 − f_i\\) of a mistake in categorizing that item. It reaches its minimum (zero) when all cases in the node fall into a single target category. To compute Gini impurity for a set of m items, suppose \\(i ∈ {1, 2, ..., m}\\), and let \\(f_i\\) be the fraction of items labeled with value \\(i\\) in the set. \\[ I_{G}(f)=\\sum _{i=1}^{m}f_{i}(1-f_{i})=\\sum _{i=1}^{m}(f_{i}-{f_{i}}^{2})=\\sum _{i=1}^{m}f_{i}-\\sum _{i=1}^{m}{f_{i}}^{2}=1-\\sum _{i=1}^{m}{f_{i}}^{2}=\\sum _{i\\neq k}f_{i}f_{k}\\] 1.5.2 Entropy Entropy, \\(H(S)\\), is a measure of the amount of uncertainty in the (data) set \\(S\\) (i.e. entropy characterizes the (data) set \\(S\\)). \\[ H(S)=-\\sum _{{x\\in X}}p(x)\\log _{{2}}p(x) \\] Where, - \\(S\\) is the current (data) set for which entropy is being calculated (changes every iteration of the ID3 algorithm) - \\(X\\) is set of classes in \\(S\\) - \\(p(x)\\) is the ratio of the number of elements in class \\(x\\) to the number of elements in set \\(S\\) When \\(H(S)=0\\), the set \\(S\\) is perfectly classified (i.e. all elements in \\(S\\) are of the same class). In ID3, entropy is calculated for each remaining attribute. The attribute with the smallest entropy is used to split the set \\(S\\) on this iteration. The higher the entropy, the higher the potential to improve the classification here. 1.5.3 Information gain Information gain \\(IG(A)\\) is the measure of the difference in entropy from before to after the set \\(S\\) is split on an attribute \\(A\\): in other words, how much uncertainty in \\(S\\) was reduced after splitting set \\(S\\) on attribute \\(A\\). \\[ IG(A,S)=H(S)-\\sum _{{t\\in T}}p(t)H(t)\\] Where, - \\(H(S)\\) is the entropy of set \\(S\\) - \\(T\\) is the set of subsets created from splitting set \\(S\\) by attribute \\(A\\) such that \\(S=\\bigcup _{{t\\in T}}t\\) - \\(p(t)\\) is the ratio of the number of elements in \\(t\\) to the number of elements in set \\(S\\) - \\(H(t)\\) is the entropy of subset \\(t\\) In ID3, information gain can be calculated (instead of entropy) for each remaining attribute. The attribute with the largest information gain is used to split the set \\(S\\) on this iteration. 1.6 Decision Boundary This is an example of a decision boundary in two dimensions of a (binary) classification tree. The black circle is the Bayes Optimal decision boundary and the blue square-ish boundary is learned by the classification tree. Figure 1.1: Source: Elements of Statistical Learning. 1.7 Missing Data CART is an algorithm that deals effectively with missing values through surrogate splits. 1.8 Visualizing Decision Trees Figure 1.2: Source: Elements of Statistical Learning. Tony Chu and Stephanie Yee designed an award-winning visualization of how decision trees work called “A Visual Introduction to Machine Learning.” Their interactive D3 visualization is available here. 1.9 CART Software in R Since it’s more common in machine learning to use trees in an ensemble, we’ll skip the code tutorial for CART in R. For reference, trees can be grown using the rpart package, among others. "],
["random-forest.html", "Chapter 2 Random Forests (RF) 2.1 Introduction 2.2 History 2.3 Bagging 2.4 Random Forest Algorithm 2.5 Decision Boundary 2.6 Random Forest by Randomization (aka “Extra-Trees”) 2.7 Out-of-Bag (OOB) Estimates 2.8 Variable Importance 2.9 Overfitting 2.10 Missing Data 2.11 Practical Uses 2.12 Resources 2.13 Random Forest Software in R 2.14 References", " Chapter 2 Random Forests (RF) Drawing by Phil Cutler. 2.1 Introduction Any tutorial on Random Forests (RF) should also include a review of decicion trees, as these are models that are ensembled together to create the Random Forest model – or put another way, the “trees that comprise the forest.” Much of the complexity and detail of the Random Forest algorithm occurs within the individual decision trees and therefore it’s important to understand decision trees to understand the RF algorithm as a whole. Therefore, before proceeding, it is recommended that you read through the accompanying Classification and Regression Trees Tutorial. 2.2 History The Random Forest algorithm is preceeded by the Random SubspaceMethod (aka “attribute bagging”), which accounts for half of the source of randomness in a Random Forest. The Random Subspace Method is an ensemble method that consists of several classifiers each operating in a subspace of the original feature space. The outputs of the models are then combined, usually by a simple majority vote. Tin Kam Ho applied the random subspace method to decision trees in 1995. Leo Breiman and Adele Culter combined Breiman’s bagging idea with the random subspace method to create a “Random Forest”, a name which is trademarked by the duo. Due to the trademark, the algorithm is sometimes called Random Decision Forests. The introduction of random forests proper was first made in a paper by Leo Breiman [1]. This paper describes a method of building a forest of uncorrelated trees using a CART like procedure, combined with randomized node optimization and bagging. In addition, this paper combines several ingredients, some previously known and some novel, which form the basis of the modern practice of random forests, in particular: Using out-of-bag error as an estimate of the generalization error. Measuring variable importance through permutation. The report also offers the first theoretical result for random forests in the form of a bound on the generalization error which depends on the strength of the trees in the forest and their correlation. Although Brieman’s implemenation of Random Forests used his CART algorithm to construct the decision trees, many modern implementations of Random Forest use entropy-based algorithms for constructing the trees. 2.3 Bagging Bagging (Bootstrap aggregating) was proposed by Leo Breiman in 1994 to improve the classification by combining classifications of randomly generated training sets. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach. Bagging or bootstrap aggregation averages a noisy fitted function, refit to many bootstrap samples to reduce it’s variance. Bagging can dramatically reduce the variance of unstable procedures (like trees), leading to improved prediction, however any simple, interpretable, model structure (like that of a tree) is lost. Bagging produces smoother decision boundaries than trees. The training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set \\(X = x_1, ..., x_n\\) with responses \\(Y = y_1, ..., y_n\\), bagging repeatedly (\\(B\\) times) selects a random sample with replacement of the training set and fits trees to these samples: For \\(b = 1, ..., B\\): 1. Sample, with replacement, \\(n\\) training examples from \\(X\\), \\(Y\\); call these \\(X_b\\), \\(Y_b\\). 2. Train a decision or regression tree, \\(f_b\\), on \\(X_b\\), \\(Y_b\\). After training, predictions for unseen samples \\(x&#39;\\) can be made by averaging the predictions from all the individual regression trees on \\(x&#39;\\): \\[ {\\hat {f}}={\\frac {1}{B}}\\sum _{b=1}^{B}{\\hat {f}}_{b}(x&#39;)\\] or by taking the majority vote in the case of decision trees. 2.4 Random Forest Algorithm The above procedure describes the original bagging algorithm for trees. Random Forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called “feature bagging”. Random Forests correct for decision trees’ habit of overfitting to their training set. Random Forest is an improvement over bagged trees that “de-correlates” the trees even further, reducing the variance. At each tree split, a random sample of \\(m\\) features is drawn and only those \\(m\\) features are considered for splitting. Typically \\(m = \\sqrt{p}\\) or \\(\\log_2p\\) where \\(p\\) is the original number of features. For each tree gown on a bootstrap sample, the error rates for observations left out of the bootstrap sample is monitored. This is called the “out-of- bag” or OOB error rate. Each tree has the same (statistical) expectation, so increasing the number of trees does not alter the bias of bagging or the Random Forest algorithm. 2.5 Decision Boundary This is an example of a decision boundary in two dimensions of a (binary) classification Random Forest. The black circle is the Bayes Optimal decision boundary and the blue square-ish boundary is learned by the classification tree. Source: Elements of Statistical Learning 2.6 Random Forest by Randomization (aka “Extra-Trees”) In Extremely Randomized Trees (aka Extra- Trees) [2], randomness goes one step further in the way splits are computed. As in Random Forests, a random subset of candidate features is used, but instead of looking for the best split, thresholds (for the split) are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias. Extremely Randomized Trees is implemented in the extraTrees R package and also available in the h2o R package as part of the h2o.randomForest() function via the histogram_type = &quot;Random&quot; argument. 2.7 Out-of-Bag (OOB) Estimates In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows: Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree. Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees. At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n averaged over all cases is the oob error estimate. This has proven to be unbiased in many tests. 2.8 Variable Importance In every tree grown in the forest, put down the OOB cases and count the number of votes cast for the correct class. Now randomly permute the values of variable m in the oob cases and put these cases down the tree. Subtract the number of votes for the correct class in the variable-\\(m\\)-permuted OOB data from the number of votes for the correct class in the untouched OOB data. The average of this number over all trees in the forest is the raw importance score for variable \\(m\\). If the values of this score from tree to tree are independent, then the standard error can be computed by a standard computation. The correlations of these scores between trees have been computed for a number of data sets and proved to be quite low, therefore we compute standard errors in the classical way, divide the raw score by its standard error to get a \\(z\\)-score, ands assign a significance level to the \\(z\\)-score assuming normality. If the number of variables is very large, forests can be run once with all the variables, then run again using only the most important variables from the first run. For each case, consider all the trees for which it is oob. Subtract the percentage of votes for the correct class in the variable-\\(m\\)-permuted OOB data from the percentage of votes for the correct class in the untouched OOB data. This is the local importance score for variable m for this case. Variable importance in Extremely Randomized Trees is explained here. 2.9 Overfitting Leo Brieman famously claimed that “Random Forests do not overfit.” This is perhaps not exactly the case, however they are certainly more robust to overfitting than a Gradient Boosting Machine (GBM). Random Forests can be overfit by growing trees that are “too deep”, for example. However, it is hard to overfit a Random Forest by adding more trees to the forest – typically that will increase accuracy (at the expense of computation time). 2.10 Missing Data Missing values do not neccessarily have to be imputed in a Random Forest implemenation, although some software packages will require it. 2.11 Practical Uses Here is a short article called, The Unreasonable Effectiveness of Random Forests, by Ahmed El Deeb, about the utility of Random Forests. It summarizes some of the algorithm’s pros and cons nicely. 2.12 Resources Gilles Louppe - Understanding Random Forests (PhD Dissertation) (pdf) Gilles Louppe - Understanding Random Forests: From Theory to Practice (slides) Trevor Hastie - Gradient Boosting &amp; Random Forests at H2O World 2014 (YouTube) Mark Landry - Gradient Boosting Method and Random Forest at H2O World 2015 (YouTube) 2.13 Random Forest Software in R The oldest and most well known implementation of the Random Forest algorithm in R is the randomForest package. There are also a number of packages that implement variants of the algorithm, and in the past few years, there have been several “big data” focused implementations contributed to the R ecosystem as well. Here is a non-comprehensive list: randomForest::randomForest h2o::h2o.randomForest DistributedR::hpdRF_parallelForest party::cForest: A random forest variant for response variables measured at arbitrary scales based on conditional inference trees. randomForestSRC implements a unified treatment of Breiman’s random forests for survival, regression and classification problems. quantregForest can regress quantiles of a numeric response on exploratory variables via a random forest approach. ranger Rborist The caret package wraps a number of different Random Forest packages in R (full list here): Conditional Inference Random Forest (party::cForest) Oblique Random Forest (obliqueRF) Parallel Random Forest (randomForest + foreach) Random Ferns (rFerns) Random Forest (randomForest) Random Forest (ranger) Quantile Random Forest (quantregForest) Random Forest by Randomization (extraTrees) Random Forest Rule-Based Model (inTrees) Random Forest with Additional Feature Selection (Boruta) Regularized Random Forest (RRF) Rotation Forest (rotationForest) Weighted Subspace Random Forest (wsrf) The mlr package wraps a number of different Random Forest packages in R: Conditional Inference Random Forest (party::cForest) Rotation Forest (rotationForest) Parallel Forest (ParallelForest) Survival Forest (randomForestSRC) Random Ferns (rFerns) Random Forest (randomForest) Random Forest (ranger) Synthetic Random Forest (randomForestSRC) Random Uniform Forest (randomUniformForest) Since there are so many different Random Forest implementations available, there have been several benchmarks to compare the performance of popular implementations, including implementations outside of R. A few examples: Benchmarking Random Forest Classification by Erin LeDell, 2013 Benchmarking Random Forest Implementations by Szilard Pafka, 2015 Ranger publication by Marvin N. Wright and Andreas Ziegler, 2015 2.13.1 randomForest Authors: Fortran original by LeoBreiman and Adele Cutler, R port by AndyLiaw and Matthew Wiener. Backend: Fortran Features: This package wraps the original Fortran code by Leo Breiman and Adele Culter and is probably the most widely known/used implemenation in R. Single-threaded. Although it’s single-threaded, smaller forests can be trained in parallel by writing custom foreach or parall el code, then combined into a bigger forest using the randomForest::combine() function. Row weights unimplemented (been on the wishlist for as long as I can remember). Uses CART trees split by Gini Impurity. Categorical predictors are allowed to have up to 53 categories. Multinomial response can have no more than 32 categories. Supports R formula interface (but I’ve read some reports that claim it’s slower when the formula interface is used). GPL-2/3 Licensed. # randomForest example # install.packages(&quot;randomForest&quot;) # install.packages(&quot;cvAUC&quot;) library(randomForest) library(cvAUC) # Load binary-response dataset train &lt;- data.table::fread(&quot;https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv&quot;) test &lt;- data.table::fread(&quot;https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv&quot;) # convert to regular data frames train &lt;- as.data.frame(train) test &lt;- as.data.frame(test) # Dimensions dim(train) ## [1] 10000 29 dim(test) ## [1] 5000 29 # Columns names(train) ## [1] &quot;response&quot; &quot;x1&quot; &quot;x2&quot; &quot;x3&quot; &quot;x4&quot; &quot;x5&quot; ## [7] &quot;x6&quot; &quot;x7&quot; &quot;x8&quot; &quot;x9&quot; &quot;x10&quot; &quot;x11&quot; ## [13] &quot;x12&quot; &quot;x13&quot; &quot;x14&quot; &quot;x15&quot; &quot;x16&quot; &quot;x17&quot; ## [19] &quot;x18&quot; &quot;x19&quot; &quot;x20&quot; &quot;x21&quot; &quot;x22&quot; &quot;x23&quot; ## [25] &quot;x24&quot; &quot;x25&quot; &quot;x26&quot; &quot;x27&quot; &quot;x28&quot; # Identity the response column ycol &lt;- &quot;response&quot; # Identify the predictor columns xcols &lt;- setdiff(names(train), ycol) # Convert response to factor (required by randomForest) train[,ycol] &lt;- as.factor(train[,ycol]) test[,ycol] &lt;- as.factor(test[,ycol]) # Train a default RF model with 500 trees set.seed(1) # For reproducibility system.time( model &lt;- randomForest( x = train[,xcols], y = train[,ycol], xtest = test[,xcols], ntree = 500 ) ) ## user system elapsed ## 18.460 0.052 18.566 # Generate predictions on test dataset preds &lt;- model$test$votes[, 2] labels &lt;- test[,ycol] # Compute AUC on the test set cvAUC::AUC(predictions = preds, labels = labels) ## [1] 0.7834743 2.13.2 caret method “parRF” Authors: Max Kuhn Backend: Fortran (wraps the randomForest package) This is a wrapper for the randomForest package that parallelizes the tree building. library(caret) library(doParallel) library(e1071) # set up parallel environment cl &lt;- makeCluster(4) registerDoParallel(cl) # Train a &quot;parRF&quot; model using caret model &lt;- caret::train( x = train[,xcols], y = train[,ycol], method = &quot;parRF&quot;, preProcess = NULL, weights = NULL, metric = &quot;Accuracy&quot;, maximize = TRUE, trControl = trainControl(), tuneGrid = NULL, tuneLength = 3 ) # good practice to shut down parallel environment stopCluster(cl) 2.13.3 h2o Authors: Jan Vitek, Arno Candel, H2O.ai contributors Backend: Java Features: Distributed and parallelized computation on either a single node or a multi- node cluster. Automatic early stopping based on convergence of user-specied metrics to user- specied relative tolerance. Data-distributed, which means the entire dataset does not need to fit into memory on a single node. Uses histogram approximations of continuous variables for speedup. Uses squared error to determine optimal splits. Automatic early stopping based on convergence of user-specied metrics to user- specied relative tolerance. Support for exponential families (Poisson, Gamma, Tweedie) and loss functions in addition to binomial (Bernoulli), Gaussian and multinomial distributions, such as Quantile regression (including Laplace). Grid search for hyperparameter optimization and model selection. Apache 2.0 Licensed. Model export in plain Java code for deployment in production environments. GUI for training &amp; model eval/viz (H2O Flow). Implementation details are presented in slidedecks by Michal Mahalova and Jan Vitek. #install.packages(&quot;h2o&quot;) library(h2o) #h2o.shutdown(prompt = FALSE) h2o.init(nthreads = -1) #Start a local H2O cluster using nthreads = num available cores ## ## H2O is not running yet, starting it now... ## ## Note: In case of errors look at the following log files: ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmpf8Tb8n/h2o_bradboehmke_started_from_r.out ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmpf8Tb8n/h2o_bradboehmke_started_from_r.err ## ## ## Starting H2O JVM and connecting: ... Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 3 seconds 197 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.4 ## H2O cluster version age: 28 days, 3 hours and 9 minutes ## H2O cluster name: H2O_started_from_R_bradboehmke_gpo922 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 1.78 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.4.4 (2018-03-15) # convert data to h2o objects train &lt;- as.h2o(train) ## | | | 0% | |=================================================================| 100% test &lt;- as.h2o(test) ## | | | 0% | |=================================================================| 100% # Dimensions dim(train) ## [1] 10000 29 dim(test) ## [1] 5000 29 # Columns names(train) ## [1] &quot;response&quot; &quot;x1&quot; &quot;x2&quot; &quot;x3&quot; &quot;x4&quot; &quot;x5&quot; ## [7] &quot;x6&quot; &quot;x7&quot; &quot;x8&quot; &quot;x9&quot; &quot;x10&quot; &quot;x11&quot; ## [13] &quot;x12&quot; &quot;x13&quot; &quot;x14&quot; &quot;x15&quot; &quot;x16&quot; &quot;x17&quot; ## [19] &quot;x18&quot; &quot;x19&quot; &quot;x20&quot; &quot;x21&quot; &quot;x22&quot; &quot;x23&quot; ## [25] &quot;x24&quot; &quot;x25&quot; &quot;x26&quot; &quot;x27&quot; &quot;x28&quot; # Identity the response column ycol &lt;- &quot;response&quot; # Identify the predictor columns xcols &lt;- setdiff(names(train), ycol) # Convert response to factor (required by randomForest) train[,ycol] &lt;- as.factor(train[,ycol]) test[,ycol] &lt;- as.factor(test[,ycol]) # Train a default RF model with 500 trees system.time( model &lt;- h2o.randomForest( x = xcols, y = ycol, training_frame = train, seed = 1, #for reproducibility ntrees = 500 ) ) ## | | | 0% | |= | 2% | |== | 3% | |=== | 5% | |==== | 7% | |===== | 8% | |======== | 13% | |============ | 18% | |=============== | 23% | |=================== | 29% | |====================== | 35% | |========================== | 40% | |============================== | 45% | |================================= | 51% | |==================================== | 56% | |======================================== | 62% | |=========================================== | 67% | |=============================================== | 73% | |=================================================== | 78% | |====================================================== | 84% | |========================================================== | 90% | |============================================================== | 95% | |=================================================================| 100% ## user system elapsed ## 0.570 0.027 25.513 # Compute AUC on test dataset # H2O computes many model performance metrics automatically, including AUC perf &lt;- h2o.performance(model = model, newdata = test) h2o.auc(perf) ## [1] 0.785394 # good practice to shut down h2o environment h2o.shutdown(prompt = FALSE) ## [1] TRUE 2.13.4 Rborist Authors: Mark Seligman Backend: C++ The Arborist provides a fast, open-source implementation of the Random Forest algorithm. The Arborist achieves its speed through efficient C++ code and parallel, distributed tree construction. This slidedeck provides detail about the implementation and vision of the project. Features: Began as proprietary implementation, but was open-sourced and rewritten following dissolution of venture. Project called “Aborist” but R package is called “Rborist”. A Python interface is in development. CPU based but a GPU version called Curborist (Cuda Rborist) is in development (unclear if it will be open source). Unlimited factor cardinality. Emphasizes multi-core but not multi-node. Both Python support and GPU support have been “coming soon” since summer 2015, not sure the status of the projects. GPL-2/3 licensed. 2.13.5 ranger Authors: Marvin N. Wright and Andreas Ziegler Backend: C++ Ranger is a fast implementation of random forest (Breiman 2001) or recursive partitioning, particularly suited for high dimensional data. Classification, regression, probability estimation and survival forests are supported. Classification and regression forests are implemented as in the original Random Forest (Breiman 2001), survival forests as in Random Survival Forests (Ishwaran et al. 2008). For probability estimation forests see Malley et al. (2012). Features: Multi-threaded. Direct support for GWAS (Genome-wide association study) data. Excellent speed and support for high-dimensional or wide data. Not as fast for “tall &amp; skinny” data (many rows, few columns). GPL-3 licensed. Plot from the ranger article. 2.14 References [1] [http://www.stat.berkeley.edu/~breiman/randomforest2001.pdf](http://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) [2] [P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,Machine Learning, 63(1), 3-42,2006.](http://link.springer.com/article/10.1007%2Fs10994-006-6226-1) [3] [http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf) "],
["gradient-boosting-machines.html", "Chapter 3 Gradient Boosting Machines (GBM) 3.1 Introduction 3.2 History 3.3 Gradient Boosting 3.4 Stagewise Additive Modeling 3.5 AdaBoost 3.6 Gradient Boosting Algorithm 3.7 Stochastic GBM 3.8 Practical Tips 3.9 Resources 3.10 GBM Software in R 3.11 References", " Chapter 3 Gradient Boosting Machines (GBM) Image Source: brucecompany.com 3.1 Introduction Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in an iterative fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. It is recommended that you read through the accompanying Classification and Regression Trees Tutorial for an overview of decision trees. 3.2 History Boosting is one of the most powerful learning ideas introduced in the last twenty years. It was originally designed for classification problems, but it can be extended to regression as well. The motivation for boosting was a procedure that combines the outputs of many “weak” classifiers to produce a powerful “committee.” A weak classifier (e.g. decision tree) is one whose error rate is only slightly better than random guessing. AdaBoost short for “Adaptive Boosting”, is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire in 1996, which is now considered to be a special case of Gradient Boosting. There are some differences between the AdaBoost algorithm and modern Gradient Boosting. In the AdaBoost algorithm, the “shortcomings” of existing weak learners are identified by high-weight data points, however in Gradient Boosting, the shortcomings are identified by gradients. The idea of gradient boosting originated in the observation by Leo Breiman that boosting can be interpreted as an optimization algorithm on a suitable cost function. Explicit regression gradient boosting algorithms were subsequently developed by Jerome H. Friedman, simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean. The latter two papers introduced the abstract view of boosting algorithms as iterative functional gradient descent algorithms. That is, algorithms that optimize a cost function over function space by iteratively choosing a function (weak hypothesis) that points in the negative gradient direction. This functional gradient view of boosting has led to the development of boosting algorithms in many areas of machine learning and statistics beyond regression and classification. In general, in terms of model performance, we have the following heirarchy: \\[Boosting &gt; Random \\: Forest &gt; Bagging &gt; Single \\: Tree\\] 3.3 Gradient Boosting Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. The purpose of boosting is to sequentially apply the weak classification algorithm to repeatedly modified versions of the data, thereby producing a sequence of weak classifiers \\(G_m(x)\\), \\(m = 1, 2, ... , M\\). 3.4 Stagewise Additive Modeling Boosting builds an additive model: \\[F(x) = \\sum_{m=1}^M \\beta_m b(x; \\gamma_m)\\] where \\(b(x; \\gamma_m)\\) is a tree and \\(\\gamma_m\\) parameterizes the splits. With boosting, the parameters, \\((\\beta_m, \\gamma_m)\\) are fit in a stagewise fashion. This slows the process down, and overfits less quickly. 3.5 AdaBoost AdaBoost builds an additive logistic regression model by stagewise fitting. AdaBoost uses an exponential loss function of the form, \\(L(y, F(x)) = exp(-yF(x))\\), similar to the negative binomial log-likelihood loss. The principal attraction of the exponential loss in the context of additive modeling is computational; it leads to the simple modular reweighting Instead of fitting trees to residuals, the special form of the exponential loss function in AdaBoost leads to fitting trees to weighted versions of the original data. Source: Elements of Statistical Learning Source: Elements of Statistical Learning 3.6 Gradient Boosting Algorithm The idea of gradient boosting originated in the observation by Leo Breiman that boosting can be interpreted as an optimization algorithm on a suitable cost function. Explicit regression gradient boosting algorithms were subsequently developed by Jerome H. Friedman simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean. The latter two papers introduced the abstract view of boosting algorithms as iterative functional gradient descent algorithms. That is, algorithms that optimize a cost function over function space by iteratively choosing a function (weak hypothesis) that points in the negative gradient direction. This functional gradient view of boosting has led to the development of boosting algorithms in many areas of machine learning and statistics beyond regression and classification. Friedman’s Gradient Boosting Algorithm for a generic loss function, \\(L(y_i, \\gamma)\\): Source: Elements of Statistical Learning 3.6.1 Loss Functions and Gradients Source: Elements of Statistical Learning The optimal number of iterations, T, and the learning rate, λ, depend on each other. 3.7 Stochastic GBM Stochastic Gradient Boosting (Friedman, 2002) proposed the stochastic gradient boosting algorithm that simply samples uniformly without replacement from the dataset before estimating the next gradient step. He found that this additional step greatly improved performance. 3.8 Practical Tips It’s more common to grow shorter trees (“shrubs” or “stumps”) in GBM than you do in Random Forest. It’s useful to try a variety of column sample (and column sample per tree) rates. Don’t assume that the set of optimal tuning parameters for one implementation of GBM will carry over and also be optimal in a different GBM implementation. 3.9 Resources Trevor Hastie - Gradient Boosting &amp; Random Forests at H2O World 2014 (YouTube) Trevor Hastie - Data Science of GBM (2013) (slides) Mark Landry - Gradient Boosting Method and Random Forest at H2O World 2015 (YouTube) Peter Prettenhofer - Gradient Boosted Regression Trees in scikit-learn at PyData London 2014 (YouTube) Alexey Natekin1 and Alois Knoll - Gradient boosting machines, a tutorial (blog post) 3.10 GBM Software in R This is not a comprehensive list of GBM software in R, however, we detail a few of the most popular implementations below: gbm, xgboost and h2o. The CRAN Machine Learning Task View lists the following projects as well. The Hinge-loss is optimized by the boosting implementation in package bst. Package GAMBoost can be used to fit generalized additive models by a boosting algorithm. An extensible boosting framework for generalized linear, additive and nonparametric models is available in package mboost. Likelihood- based boosting for Cox models is implemented in CoxBoost and for mixed models in GMMBoost. GAMLSS models can be fitted using boosting by gamboostLSS. 3.10.1 gbm Authors: Originally written by Greg Ridgeway, added to by various authors, currently maintained by Harry Southworth Backend: C++ The gbm R package is an implementation of extensions to Freund and Schapire’s AdaBoost algorithm and Friedman’s gradient boosting machine. This is the original R implementation of GBM. A presentation is available here by Mark Landry. Features: Stochastic GBM. Supports up to 1024 factor levels. Supports Classification and regression trees. Includes regression methods for: least squares absolute loss t-distribution loss quantile regression logistic multinomial logistic Poisson Cox proportional hazards partial likelihood AdaBoost exponential loss Huberized hinge loss Learning to Rank measures (LambdaMart) Out-of-bag estimator for the optimal number of iterations is provided. Easy to overfit since early stopping functionality is not automated in this package. If internal cross-validation is used, this can be parallelized to all cores on the machine. Currently undergoing a major refactoring &amp; rewrite (and has been for some time). GPL-2/3 License. #install.packages(&quot;gbm&quot;) #install.packages(&quot;cvAUC&quot;) library(gbm) library(cvAUC) # Load 2-class HIGGS dataset train &lt;- data.table::fread(&quot;https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv&quot;) test &lt;- data.table::fread(&quot;https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv&quot;) set.seed(1) model &lt;- gbm( formula = response ~ ., distribution = &quot;bernoulli&quot;, data = train, n.trees = 70, interaction.depth = 5, shrinkage = 0.3, bag.fraction = 0.5, train.fraction = 1.0, n.cores = NULL # will use all cores by default ) print(model) ## gbm(formula = response ~ ., distribution = &quot;bernoulli&quot;, data = train, ## n.trees = 70, interaction.depth = 5, shrinkage = 0.3, bag.fraction = 0.5, ## train.fraction = 1, n.cores = NULL) ## A gradient boosted model with bernoulli loss function. ## 70 iterations were performed. ## There were 28 predictors of which 28 had non-zero influence. # Generate predictions on test dataset preds &lt;- predict(model, newdata = test, n.trees = 70) labels &lt;- test[,&quot;response&quot;] # Compute AUC on the test set cvAUC::AUC(predictions = preds, labels = labels) ## [1] 0.7741609 3.10.2 xgboost Authors: Tianqi Chen, Tong He, Michael Benesty Backend: C++ The xgboost R package provides an R API to “Extreme Gradient Boosting”, which is an efficient implementation of gradient boosting framework. Parameter tuning guide and more resources here. The xgboost package is quite popular on Kaggle for data mining competitions. Features: Stochastic GBM with column and row sampling (per split and per tree) for better generalization. Includes efficient linear model solver and tree learning algorithms. Parallel computation on a single machine. Supports various objective functions, including regression, classification and ranking. The package is made to be extensible, so that users are also allowed to define their own objectives easily. Apache 2.0 License. #install.packages(&quot;xgboost&quot;) #install.packages(&quot;cvAUC&quot;) library(xgboost) library(Matrix) library(cvAUC) # convert data.table to data frames train &lt;- as.data.frame(train) test &lt;- as.data.frame(test) # Set seed because we column-sample set.seed(1) y &lt;- &quot;response&quot; train.mx &lt;- sparse.model.matrix(response ~ ., train)[,-1] test.mx &lt;- sparse.model.matrix(response ~ ., test)[,-1] dtrain &lt;- xgb.DMatrix(train.mx, label = train[,y]) dtest &lt;- xgb.DMatrix(test.mx, label = test[,y]) train.gdbt &lt;- xgb.train( params = list( objective = &quot;binary:logistic&quot;, #num_class = 2, #eval_metric = &quot;mlogloss&quot;, eta = 0.3, max_depth = 5, subsample = 1, colsample_bytree = 0.5), data = dtrain, nrounds = 70, watchlist = list(train = dtrain, test = dtest), verbose = FALSE # turn on to see modelling progress ) # Generate predictions on test dataset preds &lt;- predict(train.gdbt, newdata = dtest) labels &lt;- test[,y] # Compute AUC on the test set cvAUC::AUC(predictions = preds, labels = labels) ## [1] 0.6367853 #Advanced functionality of xgboost #install.packages(&quot;Ckmeans.1d.dp&quot;) library(Ckmeans.1d.dp) # Compute feature importance matrix names &lt;- dimnames(data.matrix(train[,-1]))[[2]] importance_matrix &lt;- xgb.importance(names, model = train.gdbt) # Plot feature importance xgb.plot.importance(importance_matrix[1:10,]) 3.10.3 h2o Authors: Arno Candel, Cliff Click, H2O.ai contributors Backend: Java H2O GBM Tuning guide by Arno Candel and H2O GBM Vignette. Features: Distributed and parallelized computation on either a single node or a multi- node cluster. Automatic early stopping based on convergence of user-specied metrics to user- specied relative tolerance. Stochastic GBM with column and row sampling (per split and per tree) for better generalization. Support for exponential families (Poisson, Gamma, Tweedie) and loss functions in addition to binomial (Bernoulli), Gaussian and multinomial distributions, such as Quantile regression (including Laplace). Grid search for hyperparameter optimization and model selection. Data-distributed, which means the entire dataset does not need to fit into memory on a single node, hence scales to any size training set. Uses histogram approximations of continuous variables for speedup. Uses dynamic binning - bin limits are reset at each tree level based on the split bins’ min and max values discovered during the last pass. Uses squared error to determine optimal splits. Distributed implementation details outlined in a blog post by Cliff Click. Unlimited factor levels. Multiclass trees (one for each class) built in parallel with each other. Apache 2.0 Licensed. Model export in plain Java code for deployment in production environments. GUI for training &amp; model eval/viz (H2O Flow). #install.packages(&quot;h2o&quot;) library(h2o) h2o.init(nthreads = -1) #Start a local H2O cluster using nthreads = num available cores ## ## H2O is not running yet, starting it now... ## ## Note: In case of errors look at the following log files: ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//RtmpjhXrun/h2o_bradboehmke_started_from_r.out ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//RtmpjhXrun/h2o_bradboehmke_started_from_r.err ## ## ## Starting H2O JVM and connecting: ... Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 3 seconds 235 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.4 ## H2O cluster version age: 28 days, 3 hours and 9 minutes ## H2O cluster name: H2O_started_from_R_bradboehmke_ukx197 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 1.78 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.4.4 (2018-03-15) # Load 10-class MNIST dataset train &lt;- read.csv(&quot;https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv&quot;) test &lt;- read.csv(&quot;https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv&quot;) print(dim(train)) ## [1] 10000 29 print(dim(test)) ## [1] 5000 29 # Identity the response column y &lt;- &quot;response&quot; # Identify the predictor columns x &lt;- setdiff(names(train), y) # Convert response to factor train[,y] &lt;- as.factor(train[,y]) test[,y] &lt;- as.factor(test[,y]) # convert train and test data frames to h2o objects train.h2o &lt;- as.h2o(train) ## | | | 0% | |=================================================================| 100% test.h2o &lt;- as.h2o(test) ## | | | 0% | |=================================================================| 100% # Train an H2O GBM model model &lt;- h2o.gbm( x = x, y = y, training_frame = train.h2o, ntrees = 70, learn_rate = 0.3, sample_rate = 1.0, max_depth = 5, col_sample_rate_per_tree = 0.5, seed = 1 ) ## | | | 0% | |==== | 6% | |========== | 16% | |============= | 20% | |=============== | 23% | |======================================================== | 86% | |=================================================================| 100% # Get model performance on a test set perf &lt;- h2o.performance(model, test.h2o) print(perf) ## H2OBinomialMetrics: gbm ## ## MSE: 0.1938163 ## RMSE: 0.4402457 ## LogLoss: 0.5701889 ## Mean Per-Class Error: 0.3250041 ## AUC: 0.7735961 ## Gini: 0.5471923 ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## 0 1 Error Rate ## 0 1112 1203 0.519654 =1203/2315 ## 1 350 2335 0.130354 =350/2685 ## Totals 1462 3538 0.310600 =1553/5000 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.345501 0.750442 273 ## 2 max f2 0.114664 0.861856 363 ## 3 max f0point5 0.582133 0.733454 171 ## 4 max accuracy 0.429572 0.704600 237 ## 5 max precision 0.948823 0.962963 13 ## 6 max recall 0.018449 1.000000 397 ## 7 max specificity 0.995085 0.999568 0 ## 8 max absolute_mcc 0.582133 0.407274 171 ## 9 max min_per_class_accuracy 0.514547 0.702808 200 ## 10 max mean_per_class_accuracy 0.514547 0.702987 200 ## ## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` # To retreive individual metrics h2o.auc(perf) ## [1] 0.7735961 # Print confusion matrix h2o.confusionMatrix(perf) ## Confusion Matrix (vertical: actual; across: predicted) for max f1 @ threshold = 0.345501053865514: ## 0 1 Error Rate ## 0 1112 1203 0.519654 =1203/2315 ## 1 350 2335 0.130354 =350/2685 ## Totals 1462 3538 0.310600 =1553/5000 # Plot scoring history over time plot(model) # Retreive feature importance vi &lt;- h2o.varimp(model) vi[1:10,] ## Variable Importances: ## variable relative_importance scaled_importance percentage ## 1 x26 541.335571 1.000000 0.200106 ## 2 x28 229.342468 0.423660 0.084777 ## 3 x25 202.556091 0.374178 0.074875 ## 4 x6 158.774506 0.293301 0.058691 ## 5 x23 154.050354 0.284575 0.056945 ## 6 x27 150.640320 0.278275 0.055684 ## 7 x4 137.782974 0.254524 0.050932 ## 8 x10 111.809761 0.206544 0.041331 ## 9 x1 105.712791 0.195281 0.039077 ## 10 x22 98.462402 0.181888 0.036397 # Plot feature importance barplot( vi$scaled_importance, names.arg = vi$variable, space = 1, las = 2, main = &quot;Variable Importance: H2O GBM&quot; ) Note that all models, data and model metrics can be viewed via the H2O Flow GUI, which should already be running since you started the H2O cluster with h2o.init(). # Early stopping example # Keep in mind that when you use early stopping, you should pass a validation set # Since the validation set is used to detmine the stopping point, a separate test set should be used for model eval # fit &lt;- h2o.gbm( # x = x, # y = y, # training_frame = train, # model_id = &quot;gbm_fit3&quot;, # validation_frame = valid, #only used if stopping_rounds &gt; 0 # ntrees = 500, # score_tree_interval = 5, #used for early stopping # stopping_rounds = 3, #used for early stopping # stopping_metric = &quot;misclassification&quot;, #used for early stopping # stopping_tolerance = 0.0005, #used for early stopping # seed = 1 # ) # GBM hyperparamters gbm_params &lt;- list( learn_rate = seq(0.01, 0.1, 0.01), max_depth = seq(2, 10, 1), sample_rate = seq(0.5, 1.0, 0.1), col_sample_rate = seq(0.1, 1.0, 0.1) ) search_criteria &lt;- list( strategy = &quot;RandomDiscrete&quot;, max_models = 20 ) # Train and validate a grid of GBMs gbm_grid &lt;- h2o.grid( &quot;gbm&quot;, x = x, y = y, grid_id = &quot;gbm_grid&quot;, training_frame = train.h2o, validation_frame = test.h2o, #test frame will only be used to calculate metrics ntrees = 70, seed = 1, hyper_params = gbm_params, search_criteria = search_criteria ) ## | | | 0% | |= | 1% | |== | 3% | |=== | 4% | |==== | 6% | |===== | 8% | |======= | 10% | |======== | 12% | |========= | 14% | |========== | 16% | |============= | 19% | |============== | 22% | |=============== | 23% | |================= | 26% | |=================== | 29% | |===================== | 33% | |======================= | 36% | |========================= | 38% | |========================== | 40% | |=========================== | 42% | |============================= | 44% | |=============================== | 47% | |================================ | 50% | |=================================== | 53% | |==================================== | 56% | |====================================== | 58% | |======================================== | 61% | |========================================= | 63% | |========================================== | 65% | |=========================================== | 67% | |============================================= | 70% | |============================================== | 71% | |================================================ | 73% | |================================================= | 75% | |================================================== | 78% | |===================================================== | 81% | |======================================================= | 85% | |========================================================= | 87% | |=========================================================== | 91% | |============================================================= | 94% | |=============================================================== | 97% | |=================================================================| 100% gbm_gridperf &lt;- h2o.getGrid( grid_id = &quot;gbm_grid&quot;, sort_by = &quot;auc&quot;, decreasing = TRUE ) print(gbm_gridperf) ## H2O Grid Details ## ================ ## ## Grid ID: gbm_grid ## Used hyper parameters: ## - col_sample_rate ## - learn_rate ## - max_depth ## - sample_rate ## Number of models: 20 ## Number of failed models: 0 ## ## Hyper-Parameter Search Summary: ordered by decreasing auc ## col_sample_rate learn_rate max_depth sample_rate model_ids ## 1 0.7 0.07 6 0.5 gbm_grid_model_0 ## 2 0.5 0.08 9 1.0 gbm_grid_model_4 ## 3 0.3 0.07 9 1.0 gbm_grid_model_2 ## 4 0.9 0.05 6 0.9 gbm_grid_model_12 ## 5 1.0 0.04 8 0.8 gbm_grid_model_14 ## 6 0.5 0.05 5 0.8 gbm_grid_model_11 ## 7 0.3 0.01 8 1.0 gbm_grid_model_8 ## 8 0.6 0.1 3 0.5 gbm_grid_model_16 ## 9 0.6 0.04 4 1.0 gbm_grid_model_1 ## 10 1.0 0.03 5 1.0 gbm_grid_model_7 ## 11 0.3 0.01 6 1.0 gbm_grid_model_13 ## 12 0.9 0.03 3 0.6 gbm_grid_model_19 ## 13 0.4 0.08 2 0.8 gbm_grid_model_9 ## 14 0.6 0.04 2 0.9 gbm_grid_model_6 ## 15 0.2 0.06 2 0.6 gbm_grid_model_3 ## 16 0.1 0.04 3 0.5 gbm_grid_model_18 ## 17 0.1 0.08 2 1.0 gbm_grid_model_15 ## 18 0.1 0.03 3 0.9 gbm_grid_model_17 ## 19 0.4 0.02 2 0.6 gbm_grid_model_5 ## 20 0.3 0.01 2 0.9 gbm_grid_model_10 ## auc ## 1 0.7839301615647285 ## 2 0.7838318632833396 ## 3 0.7832302488426625 ## 4 0.7830467640801027 ## 5 0.7826461704292708 ## 6 0.7791593164166978 ## 7 0.7779804610044604 ## 8 0.7740917262931815 ## 9 0.7724465573480378 ## 10 0.7719307729124687 ## 11 0.766567885742325 ## 12 0.7575386013811632 ## 13 0.7557383431671836 ## 14 0.7433997369595907 ## 15 0.7429429958452486 ## 16 0.7402734976732588 ## 17 0.7367849544103511 ## 18 0.7363177560320314 ## 19 0.7226789097095696 ## 20 0.7201266937751125 The grid search helped a lot. The first model we trained only had a 0.774 test set AUC, but the top GBM in our grid has a test set AUC of 0.786. More information about grid search is available in the H2O grid search R tutorial. # good practice to shut down h2o environment h2o.shutdown(prompt = FALSE) ## [1] TRUE 3.11 References [1] [Friedman, Jerome H. Greedy function approximation: A gradient boostingmachine. Ann. Statist. 29 (2001), no. 5, 1189–1232. doi:10.1214/aos/1013203451.http://projecteuclid.org/euclid.aos/1013203451.](http://projecteuclid.org/DPubS?verb=Display&amp;version=1.0&amp;service=UI&amp;handle=euclid.aos/1013203451&amp;page=record) "],
["generalized-linear-models.html", "Chapter 4 Generalized Linear Models (GLM) 4.1 Introduction 4.2 Linear Models 4.3 Regularization 4.4 Other Solvers 4.5 Data Preprocessing 4.6 GLM Software in R 4.7 Regularized GLM in R 4.8 References", " Chapter 4 Generalized Linear Models (GLM) Image Source: Wikipedia 4.1 Introduction Linear Models are one of the oldest and most well known statistical prediction algorithms which nowdays is often categorized as a “machine learning algorithm.” Generalized LinearModels (GLMs) are are a framework for modeling a response variable \\(y\\) that is bounded or discrete. Generalized linear models allow for an arbitrary link function \\(g\\) that relates the mean of the response variable to the predictors, i.e. \\(E(y) = g(β′x)\\). The link function is often related to the distribution of the response, and in particular it typically has the effect of transforming between, \\((-\\infty ,\\infty )\\), the range of the linear predictor, and the range of the response variable (e.g. \\([0,1]\\)). [1] Therefore, GLMs allow for response variables that have error distribution models other than a normal distribution. Some common examples of GLMs are: Poisson regression for count data. Logistic regression and probit regression for binary data. Multinomial logistic regression and multinomial probit regression for categorical data. Ordered probit regression for ordinal data. 4.2 Linear Models In a linear model, given a vector of inputs, \\(X^T = (X_1, X_2, ..., X_p)\\), we predict the output \\(Y\\) via the model: \\[\\hat{Y} = \\hat{\\beta}_0 + \\sum_{j=1}^p X_j \\hat{\\beta}_j\\] The term \\(\\hat{\\beta}_0\\) is the intercept, also known as the bias in machine learning. Often it is convenient to include the constant variable \\(1\\) in \\(X\\), include \\(\\beta_0\\) in the vector of coefficients \\(\\hat{\\beta}\\), and then write the linear model in vector form as an inner product, \\[\\hat{Y} = X^T\\hat{\\beta},\\] where \\(X^T\\) denotes the transpose of the design matrix. We will review the case where \\(Y\\) is a scalar, however, in general \\(Y\\) can have more than one dimension. Viewed as a function over the \\(p\\)-dimensional input space, \\(f(X) = X^T\\beta\\) is linear, and the gradient, \\(f′(X) = \\beta\\), is a vector in input space that points in the steepest uphill direction. 4.2.1 Ordinary Least Squares (OLS) There are many different methods to fitting a linear model, but the most simple and popular method is Ordinary LeastSquares (OLS). The OLS method minimizes the residual sum ofsquares (RSS), and leads to a closed-form expression for the estimated value of the unknown parameter \\(\\beta\\). \\[RSS(\\beta) = \\sum_{i=1}^n (y_i - x_i^T\\beta)^2\\] \\(RSS(\\beta)\\) is a quadradic function of the parameters, and hence its minimum always exists, but may not be unique. The solution is easiest to characterize in matrix notation: \\[RSS(\\beta) = (\\boldsymbol{y} - \\boldsymbol{X}\\beta)^T(\\boldsymbol{y} - \\boldsymbol{X}\\beta)\\] where \\(\\boldsymbol{X}\\) is an \\(n \\times p\\) matrix with each row an input vector, and \\(\\boldsymbol{y}\\) is a vector of length \\(n\\) representing the response in the training set. Differentiating with respect to \\(\\beta\\), we get the normal equations, \\[\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X}\\beta) = 0\\] If \\(\\boldsymbol{X}^T\\boldsymbol{X}\\) is nonsingular, then the unique solution is given by: \\[\\hat{\\beta} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}\\] The fitted value at the \\(i^{th}\\) input, \\(x_i\\) is \\(\\hat{y}_i = \\hat{y}(x_i) = x_i^T\\hat{\\beta}\\). To solve this equation for \\(\\beta\\), we must invert a matrix, \\(\\boldsymbol{X}^T\\boldsymbol{X}\\), however it can be computationally expensive to invert this matrix directly. There are computational shortcuts for solving the normal equations available via QR or Cholesky decomposition. When dealing with large training sets, it is useful to have an understanding of the underlying computational methods in the software that you are using. Some GLM software implementations may not utilize all available computational shortcuts, costing you extra time to train your GLMs, or require you to upgrade the memory on your machine. 4.3 Regularization http://web.stanford.edu/~hastie/Papers/glmpath.pdf 4.3.1 Ridge Regression Consider a sample consisting of \\(n\\) cases, each of which consists of \\(p\\) covariates and a single outcome. Let \\(y_i\\) be the outcome and \\(X_i := ( x_1 , x_2 , … , x_p)^T\\). Then the objective of Ridge is to solve: \\[{\\displaystyle \\min _{\\beta }\\left\\{{\\frac {1}{N}}\\sum _{i=1}^{N}\\left(y_{i}-\\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j \\right)^{2}\\right\\}{\\text{ subject to }}\\sum _{j=1}^{p}\\beta _{j}^2 \\leq t.}\\] Here \\(t\\) is a prespecified free parameter that determines the amount of regularization. Ridge is also called \\(\\ell_2\\) regularization. 4.3.2 Lasso Regression Lasso (least absolute shrinkage and selection operator) (also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces. It was introduced by Robert Tibshirani in 1996 based on Leo Breiman’s Nonnegative Garrote. Lasso conveniently performs coefficient shrinkage comparable to the ridge regression as well as variable selection by reducing coefficients to zero. By sacrificing a small amount of bias in the predicted response variable in order to decrease variance, the lasso achieves improved predictive accuracy compared with ordinary least squares (OLS) models, particularly with data containing highly correlated predictor variables or in over determined data where \\(p&gt;n\\). Then the objective of Lasso is to solve: \\[{\\displaystyle \\min _{\\beta }\\left\\{{\\frac {1}{N}}\\sum _{i=1}^{N}\\left(y_{i}-\\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j \\right)^{2}\\right\\}{\\text{ subject to }}\\sum _{j=1}^{p}|\\beta _{j}| \\leq t.}\\] Here \\(t\\) is a prespecified free parameter that determines the amount of regularization. Lasso is also called \\(\\ell_1\\) regularization. 4.3.3 Elastic Net Elastic Net regularization is a simple blend of Lasso and Ridge regularization. In software, this is typically controlled by an alpha parameter in between 0 and 1, where: alpha = 0.0 is Ridge regression alpha = 0.5 is a 50/50 blend of Ridge/Lasso regression alpha = 1.0 is Lasso regression 4.4 Other Solvers GLM models are trained by finding the set of parameters that maximizes the likelihood of the data. For the Gaussian family, maximum likelihood consists of minimizing the mean squared error. This has an analytical solution and can be solved with a standard method of least squares. This is also applicable when the \\(\\ell_2\\) penalty is added to the optimization. For all other families and when the \\(\\ell_1\\) penalty is included, the maximum likelihood problem has no analytical solution. Therefore an iterative method such as IRLSM, L-BFGS, the Newton method, or gradient descent, must be used. 4.4.1 Iteratively Re-weighted Least Squares (IRLS) The IRLS method is used to solve certain optimization problems with objective functions of the form: \\[{\\underset {{\\boldsymbol \\beta }}{\\operatorname {arg\\,min}}}\\sum _{{i=1}}^{n}{\\big |}y_{i}-f_{i}({\\boldsymbol \\beta }){\\big |}^{p},\\] by an iterative method in which each step involves solving a weighted least squares problem of the form: \\[{\\boldsymbol \\beta }^{{(t+1)}}={\\underset {{\\boldsymbol \\beta }}{\\operatorname {arg\\,min}}}\\sum _{{i=1}}^{n}w_{i}({\\boldsymbol \\beta }^{{(t)}}){\\big |}y_{i}-f_{i}({\\boldsymbol \\beta }){\\big |}^{2}.\\] IRLS is used to find the maximum likelihood estimates of a generalized linear model as a way of mitigating the influence of outliers in an otherwise normally-distributed data set. For example, by minimizing the least absolute error rather than the least square error. One of the advantages of IRLS over linear programming and convex programming is that it can be used with Gauss-Newton and Levenberg-Marquardt numerical algorithms. The IRL1 algorithm solves a sequence of non-smooth weighted \\(\\ell_1\\)-minimization problems, and hence can be seen as the non-smooth counterpart to the IRLS algorithm. 4.4.2 Iteratively Re-weighted Least Squares with ADMM The IRLS method with alternating direction method of multipliers (ADMM) inner solver as described in Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers by Boyd et. al to deal with the \\(\\ell_1\\) penalty. ADMM is an algorithm that solves convex optimization problems by breaking them into smaller pieces, each of which are then easier to handle. Every iteration of the algorithm consists of following steps: Generate weighted least squares problem based on previous solution, i.e. vector of weights w and response z. Compute the weighted Gram matrix XT WX and XT z vector Decompose the Gram matrix (Cholesky decomposition) and apply ADMM solver to solve the \\(\\ell_1\\) penalized least squares problem. In the H2O GLM implementation, steps 1 and 2 are performed distributively, and Step 3 is computed in parallel on a single node. The Gram matrix appraoch is very efficient for tall and narrow datasets when running lamnda search with a sparse solution. 4.4.3 Cyclical Coordinate Descent The IRLS method can also use cyclical coordinate descent in it’s inner loop (as opposed to ADMM). The glmnet package uses cyclical coordinate descent which successively optimizes the objective function over each parameter with others fixed, and cycles repeatedly until convergence. Cyclical coordinate descent methods are a natural approach for solving convex problems with \\(\\ell_1\\) or \\(\\ell_2\\) constraints, or mixtures of the two (elastic net). Each coordinate-descent step is fast, with an explicit formula for each coordinate-wise minimization. The method also exploits the sparsity of the model, spending much of its time evaluating only inner products for variables with non-zero coefficients. 4.4.4 L-BFGS Limited-memory BFGS (L-BFGS) is an optimization algorithm in the family of quasi-Newton methods that approximates the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm using a limited amount of computer memory. Due to its resulting linear memory requirement, the L-BFGS method is particularly well suited for optimization problems with a large number of variables. The method is popular among “big data” GLM implementations such as h2o::h2o.glm() (one of two available solvers) and SparkR::glm(). The L-BFGS-B algorithm is an extension of the L-BFGS algorithm to handle simple bounds on the model. 4.5 Data Preprocessing In order for the coefficients to be easily interpretable, the features must be centered and scaled (aka “normalized”). Many software packages will allow the direct input of categorical/factor columns in the training frame, however internally any categorical columns will be expaded into binary indicator variables. The caret package offers a handy utility function, caret::dummyVars (), for dummy/indicator expansion if you need to do this manually. Missing data will need to be imputed, otherwise in many GLM packages, those rows will simply be omitted from the training set at train time. For example, in the stats::glm() function there is an na.action argument which allows the user to do one of the three options: na.omit and na.exclude: observations are removed if they contain any missing values; if na.exclude is used some functions will pad residuals and predictions to the correct length by inserting NAs for omitted cases. na.pass: keep all data, including NAs na.fail: returns the object only if it contains no missing values Other GLM implementations such as h2o::glm() will impute the mean automatically (in both training and test data), unless specified by the user. 4.6 GLM Software in R There is an implementation of the standard GLM (no regularization) in the built- in “stats” package in R called glm. 4.6.1 glm Authors: The original R implementation of glm was written by Simon Davies working for Ross Ihaka at the University of Auckland, but has since been extensively re-written by members of the R Core team. The design was inspired by the S function of the same name described in Hastie &amp; Pregibon (1992). Backend: Fortran 4.6.1.1 Example Linear Regression with glm() #install.packages(&quot;caret&quot;) library(caret) data(&quot;Sacramento&quot;) # Split the data into a 70/25% train/test sets set.seed(1) idxs &lt;- caret::createDataPartition(y = Sacramento$price, p = 0.75)[[1]] train &lt;- Sacramento[idxs,] test &lt;- Sacramento[-idxs,] # Fit the GLM fit &lt;- glm( price ~ ., data = train, family = gaussian() ) summary(fit) ## ## Call: ## glm(formula = price ~ ., family = gaussian(), data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -269404 -39233 -6677 27418 279476 ## ## Coefficients: (32 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.434e+06 1.931e+07 0.385 0.700365 ## cityAUBURN 1.450e+05 7.163e+04 2.024 0.043348 * ## cityCAMERON_PARK 3.497e+04 6.341e+04 0.551 0.581536 ## cityCARMICHAEL 8.019e+04 2.699e+04 2.971 0.003078 ** ## cityCITRUS_HEIGHTS -8.352e+03 2.222e+04 -0.376 0.707161 ## cityCOOL 1.300e+05 1.015e+05 1.281 0.200691 ## cityEL_DORADO 3.534e+04 1.061e+05 0.333 0.739141 ## cityEL_DORADO_HILLS 1.243e+05 5.417e+04 2.295 0.022047 * ## cityELK_GROVE -6.302e+04 5.984e+04 -1.053 0.292688 ## cityELVERTA -5.559e+04 5.066e+04 -1.097 0.272929 ## cityFAIR_OAKS 6.136e+04 3.307e+04 1.855 0.064008 . ## cityFOLSOM 1.056e+05 4.168e+04 2.533 0.011540 * ## cityFORESTHILL 7.605e+04 1.283e+05 0.593 0.553518 ## cityGALT -9.711e+04 8.547e+04 -1.136 0.256354 ## cityGOLD_RIVER 3.459e+04 6.240e+04 0.554 0.579555 ## cityGRANITE_BAY 3.217e+05 7.939e+04 4.053 5.70e-05 *** ## cityGREENWOOD 7.445e+04 1.119e+05 0.665 0.506249 ## cityLINCOLN 3.605e+04 3.989e+04 0.904 0.366484 ## cityLOOMIS 3.754e+05 6.367e+04 5.896 6.08e-09 *** ## cityMATHER -8.306e+04 7.679e+04 -1.082 0.279856 ## cityMEADOW_VISTA 1.221e+05 1.060e+05 1.153 0.249537 ## cityNORTH_HIGHLANDS -3.655e+04 2.425e+04 -1.507 0.132220 ## cityORANGEVALE 5.875e+04 3.518e+04 1.670 0.095465 . ## cityPLACERVILLE 1.271e+05 9.188e+04 1.384 0.166998 ## cityPOLLOCK_PINES 3.648e+04 1.291e+05 0.283 0.777580 ## cityRANCHO_CORDOVA -6.894e+04 4.502e+04 -1.531 0.126222 ## cityRANCHO_MURIETA -1.675e+05 8.849e+04 -1.893 0.058780 . ## cityRIO_LINDA -2.221e+04 3.001e+04 -0.740 0.459428 ## cityROCKLIN 9.179e+04 3.562e+04 2.577 0.010205 * ## cityROSEVILLE 9.536e+04 2.678e+04 3.560 0.000398 *** ## citySACRAMENTO 1.203e+05 4.113e+04 2.926 0.003561 ** ## cityWALNUT_GROVE 4.904e+04 1.173e+05 0.418 0.676124 ## cityWEST_SACRAMENTO -7.501e+04 6.405e+04 -1.171 0.242019 ## cityWILTON 1.265e+05 7.234e+04 1.749 0.080744 . ## zipz95608 NA NA NA NA ## zipz95610 7.732e+03 3.056e+04 0.253 0.800363 ## zipz95614 NA NA NA NA ## zipz95621 NA NA NA NA ## zipz95623 NA NA NA NA ## zipz95624 -1.452e+03 2.038e+04 -0.071 0.943237 ## zipz95626 NA NA NA NA ## zipz95628 NA NA NA NA ## zipz95630 NA NA NA NA ## zipz95631 NA NA NA NA ## zipz95632 NA NA NA NA ## zipz95635 NA NA NA NA ## zipz95648 NA NA NA NA ## zipz95650 NA NA NA NA ## zipz95655 NA NA NA NA ## zipz95660 NA NA NA NA ## zipz95661 2.926e+04 3.994e+04 0.733 0.464062 ## zipz95662 NA NA NA NA ## zipz95667 NA NA NA NA ## zipz95670 5.343e+04 3.267e+04 1.635 0.102495 ## zipz95673 NA NA NA NA ## zipz95677 1.607e+04 4.510e+04 0.356 0.721739 ## zipz95678 -4.077e+04 2.838e+04 -1.437 0.151329 ## zipz95682 NA NA NA NA ## zipz95683 NA NA NA NA ## zipz95690 NA NA NA NA ## zipz95691 NA NA NA NA ## zipz95693 NA NA NA NA ## zipz95722 NA NA NA NA ## zipz95726 NA NA NA NA ## zipz95742 NA NA NA NA ## zipz95746 NA NA NA NA ## zipz95747 NA NA NA NA ## zipz95757 1.339e+04 1.874e+04 0.715 0.475149 ## zipz95758 NA NA NA NA ## zipz95762 NA NA NA NA ## zipz95765 NA NA NA NA ## zipz95811 1.067e+05 7.624e+04 1.400 0.162053 ## zipz95814 -7.275e+04 6.049e+04 -1.203 0.229549 ## zipz95815 -1.940e+05 3.691e+04 -5.256 2.02e-07 *** ## zipz95816 -2.631e+03 4.872e+04 -0.054 0.956956 ## zipz95817 -1.662e+05 4.612e+04 -3.605 0.000337 *** ## zipz95818 -5.284e+04 4.452e+04 -1.187 0.235783 ## zipz95819 1.074e+05 5.104e+04 2.104 0.035800 * ## zipz95820 -1.712e+05 3.770e+04 -4.540 6.73e-06 *** ## zipz95821 -9.943e+04 4.344e+04 -2.289 0.022420 * ## zipz95822 -1.762e+05 4.333e+04 -4.066 5.39e-05 *** ## zipz95823 -2.001e+05 4.157e+04 -4.813 1.86e-06 *** ## zipz95824 -2.056e+05 4.097e+04 -5.019 6.77e-07 *** ## zipz95825 -1.386e+05 3.719e+04 -3.727 0.000212 *** ## zipz95826 -1.435e+05 3.811e+04 -3.765 0.000182 *** ## zipz95827 -1.880e+05 4.005e+04 -4.694 3.30e-06 *** ## zipz95828 -2.005e+05 3.935e+04 -5.096 4.60e-07 *** ## zipz95829 -1.802e+05 4.471e+04 -4.030 6.25e-05 *** ## zipz95831 -1.137e+05 5.317e+04 -2.138 0.032914 * ## zipz95832 -2.242e+05 4.891e+04 -4.584 5.52e-06 *** ## zipz95833 -1.549e+05 4.048e+04 -3.827 0.000143 *** ## zipz95834 -1.510e+05 4.150e+04 -3.639 0.000297 *** ## zipz95835 -1.202e+05 4.195e+04 -2.865 0.004309 ** ## zipz95838 -1.729e+05 3.648e+04 -4.741 2.64e-06 *** ## zipz95841 -7.757e+04 5.190e+04 -1.495 0.135538 ## zipz95842 -1.651e+05 3.921e+04 -4.210 2.93e-05 *** ## zipz95843 NA NA NA NA ## zipz95864 NA NA NA NA ## beds -1.336e+04 5.016e+03 -2.664 0.007926 ** ## baths 1.314e+04 6.323e+03 2.078 0.038099 * ## sqft 1.148e+02 7.457e+00 15.388 &lt; 2e-16 *** ## typeMulti_Family 3.034e+04 2.731e+04 1.111 0.266933 ## typeResidential 4.665e+04 1.298e+04 3.594 0.000351 *** ## latitude -2.121e+05 1.895e+05 -1.120 0.263319 ## longitude -6.519e+03 1.547e+05 -0.042 0.966399 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 4622177625) ## ## Null deviance: 1.2350e+13 on 699 degrees of freedom ## Residual deviance: 2.8981e+12 on 627 degrees of freedom ## AIC: 17635 ## ## Number of Fisher Scoring iterations: 2 # Predict on the test set pred &lt;- predict(fit, newdata = test) ## Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor city has new levels DIAMOND_SPRINGS, GARDEN_VALLEY, PENRYN Above we have a slight issue. The city column has new factor levels in the test set that were not present in the training set. Even though the train and test data frames originated from a single data frame, Sacramento, and therefore have identical factor levels, we still run into this problem. Let’s take a closer look at the factor levels to see what’s going on: str(train) ## &#39;data.frame&#39;: 700 obs. of 9 variables: ## $ city : Factor w/ 37 levels &quot;ANTELOPE&quot;,&quot;AUBURN&quot;,..: 34 34 34 34 34 29 31 34 34 34 ... ## $ zip : Factor w/ 68 levels &quot;z95603&quot;,&quot;z95608&quot;,..: 52 44 44 53 65 24 25 44 51 66 ... ## $ beds : int 3 2 2 2 3 2 3 1 3 2 ... ## $ baths : num 1 1 1 1 1 2 2 1 1 2 ... ## $ sqft : int 1167 796 852 797 1122 941 1146 871 1020 1022 ... ## $ type : Factor w/ 3 levels &quot;Condo&quot;,&quot;Multi_Family&quot;,..: 3 3 3 3 1 1 3 3 3 3 ... ## $ price : int 68212 68880 69307 81900 89921 94905 98937 106852 107502 108750 ... ## $ latitude : num 38.5 38.6 38.6 38.5 38.7 ... ## $ longitude: num -121 -121 -121 -121 -121 ... str(test) ## &#39;data.frame&#39;: 232 obs. of 9 variables: ## $ city : Factor w/ 37 levels &quot;ANTELOPE&quot;,&quot;AUBURN&quot;,..: 34 34 34 34 34 1 34 24 11 10 ... ## $ zip : Factor w/ 68 levels &quot;z95603&quot;,&quot;z95608&quot;,..: 64 66 49 64 52 67 57 19 9 8 ... ## $ beds : int 2 3 3 3 3 3 3 3 3 3 ... ## $ baths : num 1 2 1 2 2 2 2 2 2 2 ... ## $ sqft : int 836 1104 1177 909 1289 1088 1248 1152 1116 1056 ... ## $ type : Factor w/ 3 levels &quot;Condo&quot;,&quot;Multi_Family&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ price : int 59222 90895 91002 100309 106250 126640 132000 134555 138750 156896 ... ## $ latitude : num 38.6 38.7 38.5 38.6 38.5 ... ## $ longitude: num -121 -121 -121 -121 -121 ... Although train and test have identical structure, not all the levels are represented in the training data. To validate this, let’s take a look at the actual unique levels that were used in the model: # Check the number of levels in the model features sapply(fit$xlevels, function(x) print(length(x))) ## [1] 34 ## [1] 65 ## [1] 3 ## city zip type ## 34 65 3 We can manually fix this by updating the xlevels element of the model. We have the same issue with zip, so we should go ahead and manually update that as well. # Update factor levels so that prediction works fit$xlevels[[&quot;city&quot;]] &lt;- union(fit$xlevels[[&quot;city&quot;]], levels(test$city)) fit$xlevels[[&quot;zip&quot;]] &lt;- union(fit$xlevels[[&quot;zip&quot;]], levels(test$zip)) # Predict on the test set pred &lt;- predict(fit, newdata = test) summary(fit) ## ## Call: ## glm(formula = price ~ ., family = gaussian(), data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -269404 -39233 -6677 27418 279476 ## ## Coefficients: (32 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.434e+06 1.931e+07 0.385 0.700365 ## cityAUBURN 1.450e+05 7.163e+04 2.024 0.043348 * ## cityCAMERON_PARK 3.497e+04 6.341e+04 0.551 0.581536 ## cityCARMICHAEL 8.019e+04 2.699e+04 2.971 0.003078 ** ## cityCITRUS_HEIGHTS -8.352e+03 2.222e+04 -0.376 0.707161 ## cityCOOL 1.300e+05 1.015e+05 1.281 0.200691 ## cityEL_DORADO 3.534e+04 1.061e+05 0.333 0.739141 ## cityEL_DORADO_HILLS 1.243e+05 5.417e+04 2.295 0.022047 * ## cityELK_GROVE -6.302e+04 5.984e+04 -1.053 0.292688 ## cityELVERTA -5.559e+04 5.066e+04 -1.097 0.272929 ## cityFAIR_OAKS 6.136e+04 3.307e+04 1.855 0.064008 . ## cityFOLSOM 1.056e+05 4.168e+04 2.533 0.011540 * ## cityFORESTHILL 7.605e+04 1.283e+05 0.593 0.553518 ## cityGALT -9.711e+04 8.547e+04 -1.136 0.256354 ## cityGOLD_RIVER 3.459e+04 6.240e+04 0.554 0.579555 ## cityGRANITE_BAY 3.217e+05 7.939e+04 4.053 5.70e-05 *** ## cityGREENWOOD 7.445e+04 1.119e+05 0.665 0.506249 ## cityLINCOLN 3.605e+04 3.989e+04 0.904 0.366484 ## cityLOOMIS 3.754e+05 6.367e+04 5.896 6.08e-09 *** ## cityMATHER -8.306e+04 7.679e+04 -1.082 0.279856 ## cityMEADOW_VISTA 1.221e+05 1.060e+05 1.153 0.249537 ## cityNORTH_HIGHLANDS -3.655e+04 2.425e+04 -1.507 0.132220 ## cityORANGEVALE 5.875e+04 3.518e+04 1.670 0.095465 . ## cityPLACERVILLE 1.271e+05 9.188e+04 1.384 0.166998 ## cityPOLLOCK_PINES 3.648e+04 1.291e+05 0.283 0.777580 ## cityRANCHO_CORDOVA -6.894e+04 4.502e+04 -1.531 0.126222 ## cityRANCHO_MURIETA -1.675e+05 8.849e+04 -1.893 0.058780 . ## cityRIO_LINDA -2.221e+04 3.001e+04 -0.740 0.459428 ## cityROCKLIN 9.179e+04 3.562e+04 2.577 0.010205 * ## cityROSEVILLE 9.536e+04 2.678e+04 3.560 0.000398 *** ## citySACRAMENTO 1.203e+05 4.113e+04 2.926 0.003561 ** ## cityWALNUT_GROVE 4.904e+04 1.173e+05 0.418 0.676124 ## cityWEST_SACRAMENTO -7.501e+04 6.405e+04 -1.171 0.242019 ## cityWILTON 1.265e+05 7.234e+04 1.749 0.080744 . ## zipz95608 NA NA NA NA ## zipz95610 7.732e+03 3.056e+04 0.253 0.800363 ## zipz95614 NA NA NA NA ## zipz95621 NA NA NA NA ## zipz95623 NA NA NA NA ## zipz95624 -1.452e+03 2.038e+04 -0.071 0.943237 ## zipz95626 NA NA NA NA ## zipz95628 NA NA NA NA ## zipz95630 NA NA NA NA ## zipz95631 NA NA NA NA ## zipz95632 NA NA NA NA ## zipz95635 NA NA NA NA ## zipz95648 NA NA NA NA ## zipz95650 NA NA NA NA ## zipz95655 NA NA NA NA ## zipz95660 NA NA NA NA ## zipz95661 2.926e+04 3.994e+04 0.733 0.464062 ## zipz95662 NA NA NA NA ## zipz95667 NA NA NA NA ## zipz95670 5.343e+04 3.267e+04 1.635 0.102495 ## zipz95673 NA NA NA NA ## zipz95677 1.607e+04 4.510e+04 0.356 0.721739 ## zipz95678 -4.077e+04 2.838e+04 -1.437 0.151329 ## zipz95682 NA NA NA NA ## zipz95683 NA NA NA NA ## zipz95690 NA NA NA NA ## zipz95691 NA NA NA NA ## zipz95693 NA NA NA NA ## zipz95722 NA NA NA NA ## zipz95726 NA NA NA NA ## zipz95742 NA NA NA NA ## zipz95746 NA NA NA NA ## zipz95747 NA NA NA NA ## zipz95757 1.339e+04 1.874e+04 0.715 0.475149 ## zipz95758 NA NA NA NA ## zipz95762 NA NA NA NA ## zipz95765 NA NA NA NA ## zipz95811 1.067e+05 7.624e+04 1.400 0.162053 ## zipz95814 -7.275e+04 6.049e+04 -1.203 0.229549 ## zipz95815 -1.940e+05 3.691e+04 -5.256 2.02e-07 *** ## zipz95816 -2.631e+03 4.872e+04 -0.054 0.956956 ## zipz95817 -1.662e+05 4.612e+04 -3.605 0.000337 *** ## zipz95818 -5.284e+04 4.452e+04 -1.187 0.235783 ## zipz95819 1.074e+05 5.104e+04 2.104 0.035800 * ## zipz95820 -1.712e+05 3.770e+04 -4.540 6.73e-06 *** ## zipz95821 -9.943e+04 4.344e+04 -2.289 0.022420 * ## zipz95822 -1.762e+05 4.333e+04 -4.066 5.39e-05 *** ## zipz95823 -2.001e+05 4.157e+04 -4.813 1.86e-06 *** ## zipz95824 -2.056e+05 4.097e+04 -5.019 6.77e-07 *** ## zipz95825 -1.386e+05 3.719e+04 -3.727 0.000212 *** ## zipz95826 -1.435e+05 3.811e+04 -3.765 0.000182 *** ## zipz95827 -1.880e+05 4.005e+04 -4.694 3.30e-06 *** ## zipz95828 -2.005e+05 3.935e+04 -5.096 4.60e-07 *** ## zipz95829 -1.802e+05 4.471e+04 -4.030 6.25e-05 *** ## zipz95831 -1.137e+05 5.317e+04 -2.138 0.032914 * ## zipz95832 -2.242e+05 4.891e+04 -4.584 5.52e-06 *** ## zipz95833 -1.549e+05 4.048e+04 -3.827 0.000143 *** ## zipz95834 -1.510e+05 4.150e+04 -3.639 0.000297 *** ## zipz95835 -1.202e+05 4.195e+04 -2.865 0.004309 ** ## zipz95838 -1.729e+05 3.648e+04 -4.741 2.64e-06 *** ## zipz95841 -7.757e+04 5.190e+04 -1.495 0.135538 ## zipz95842 -1.651e+05 3.921e+04 -4.210 2.93e-05 *** ## zipz95843 NA NA NA NA ## zipz95864 NA NA NA NA ## beds -1.336e+04 5.016e+03 -2.664 0.007926 ** ## baths 1.314e+04 6.323e+03 2.078 0.038099 * ## sqft 1.148e+02 7.457e+00 15.388 &lt; 2e-16 *** ## typeMulti_Family 3.034e+04 2.731e+04 1.111 0.266933 ## typeResidential 4.665e+04 1.298e+04 3.594 0.000351 *** ## latitude -2.121e+05 1.895e+05 -1.120 0.263319 ## longitude -6.519e+03 1.547e+05 -0.042 0.966399 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 4622177625) ## ## Null deviance: 1.2350e+13 on 699 degrees of freedom ## Residual deviance: 2.8981e+12 on 627 degrees of freedom ## AIC: 17635 ## ## Number of Fisher Scoring iterations: 2 # Compute model performance on the test set caret::R2(pred = pred, obs = test$price) ## [1] 0.03715124 caret::RMSE(pred = pred, obs = test$price) ## [1] 7173982 4.6.2 GLM in caret Now let’s run the same model using caret’s glm method to get a sense of how much easier it is to use. # Train a caret glm model fit &lt;- caret::train( form = price ~ ., data = train, trControl = trainControl(method = &quot;none&quot;), method = &quot;glm&quot;, family = gaussian() ) summary(fit$finalModel) ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -269404 -39233 -6677 27418 279476 ## ## Coefficients: (38 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.434e+06 1.931e+07 0.385 0.700365 ## cityAUBURN 1.450e+05 7.163e+04 2.024 0.043348 * ## cityCAMERON_PARK 3.497e+04 6.341e+04 0.551 0.581536 ## cityCARMICHAEL 8.019e+04 2.699e+04 2.971 0.003078 ** ## cityCITRUS_HEIGHTS -8.352e+03 2.222e+04 -0.376 0.707161 ## cityCOOL 1.300e+05 1.015e+05 1.281 0.200691 ## cityDIAMOND_SPRINGS NA NA NA NA ## cityEL_DORADO 3.534e+04 1.061e+05 0.333 0.739141 ## cityEL_DORADO_HILLS 1.243e+05 5.417e+04 2.295 0.022047 * ## cityELK_GROVE -6.302e+04 5.984e+04 -1.053 0.292688 ## cityELVERTA -5.559e+04 5.066e+04 -1.097 0.272929 ## cityFAIR_OAKS 6.136e+04 3.307e+04 1.855 0.064008 . ## cityFOLSOM 1.056e+05 4.168e+04 2.533 0.011540 * ## cityFORESTHILL 7.605e+04 1.283e+05 0.593 0.553518 ## cityGALT -9.711e+04 8.547e+04 -1.136 0.256354 ## cityGARDEN_VALLEY NA NA NA NA ## cityGOLD_RIVER 3.459e+04 6.240e+04 0.554 0.579555 ## cityGRANITE_BAY 3.217e+05 7.939e+04 4.053 5.70e-05 *** ## cityGREENWOOD 7.445e+04 1.119e+05 0.665 0.506249 ## cityLINCOLN 3.605e+04 3.989e+04 0.904 0.366484 ## cityLOOMIS 3.754e+05 6.367e+04 5.896 6.08e-09 *** ## cityMATHER -8.306e+04 7.679e+04 -1.082 0.279856 ## cityMEADOW_VISTA 1.221e+05 1.060e+05 1.153 0.249537 ## cityNORTH_HIGHLANDS -3.655e+04 2.425e+04 -1.507 0.132220 ## cityORANGEVALE 5.875e+04 3.518e+04 1.670 0.095465 . ## cityPENRYN NA NA NA NA ## cityPLACERVILLE 1.271e+05 9.188e+04 1.384 0.166998 ## cityPOLLOCK_PINES 3.648e+04 1.291e+05 0.283 0.777580 ## cityRANCHO_CORDOVA -6.894e+04 4.502e+04 -1.531 0.126222 ## cityRANCHO_MURIETA -1.675e+05 8.849e+04 -1.893 0.058780 . ## cityRIO_LINDA -2.221e+04 3.001e+04 -0.740 0.459428 ## cityROCKLIN 9.179e+04 3.562e+04 2.577 0.010205 * ## cityROSEVILLE 9.536e+04 2.678e+04 3.560 0.000398 *** ## citySACRAMENTO 1.203e+05 4.113e+04 2.926 0.003561 ** ## cityWALNUT_GROVE 4.904e+04 1.173e+05 0.418 0.676124 ## cityWEST_SACRAMENTO -7.501e+04 6.405e+04 -1.171 0.242019 ## cityWILTON 1.265e+05 7.234e+04 1.749 0.080744 . ## zipz95608 NA NA NA NA ## zipz95610 7.732e+03 3.056e+04 0.253 0.800363 ## zipz95614 NA NA NA NA ## zipz95619 NA NA NA NA ## zipz95621 NA NA NA NA ## zipz95623 NA NA NA NA ## zipz95624 -1.452e+03 2.038e+04 -0.071 0.943237 ## zipz95626 NA NA NA NA ## zipz95628 NA NA NA NA ## zipz95630 NA NA NA NA ## zipz95631 NA NA NA NA ## zipz95632 NA NA NA NA ## zipz95633 NA NA NA NA ## zipz95635 NA NA NA NA ## zipz95648 NA NA NA NA ## zipz95650 NA NA NA NA ## zipz95655 NA NA NA NA ## zipz95660 NA NA NA NA ## zipz95661 2.926e+04 3.994e+04 0.733 0.464062 ## zipz95662 NA NA NA NA ## zipz95663 NA NA NA NA ## zipz95667 NA NA NA NA ## zipz95670 5.343e+04 3.267e+04 1.635 0.102495 ## zipz95673 NA NA NA NA ## zipz95677 1.607e+04 4.510e+04 0.356 0.721739 ## zipz95678 -4.077e+04 2.838e+04 -1.437 0.151329 ## zipz95682 NA NA NA NA ## zipz95683 NA NA NA NA ## zipz95690 NA NA NA NA ## zipz95691 NA NA NA NA ## zipz95693 NA NA NA NA ## zipz95722 NA NA NA NA ## zipz95726 NA NA NA NA ## zipz95742 NA NA NA NA ## zipz95746 NA NA NA NA ## zipz95747 NA NA NA NA ## zipz95757 1.339e+04 1.874e+04 0.715 0.475149 ## zipz95758 NA NA NA NA ## zipz95762 NA NA NA NA ## zipz95765 NA NA NA NA ## zipz95811 1.067e+05 7.624e+04 1.400 0.162053 ## zipz95814 -7.275e+04 6.049e+04 -1.203 0.229549 ## zipz95815 -1.940e+05 3.691e+04 -5.256 2.02e-07 *** ## zipz95816 -2.631e+03 4.872e+04 -0.054 0.956956 ## zipz95817 -1.662e+05 4.612e+04 -3.605 0.000337 *** ## zipz95818 -5.284e+04 4.452e+04 -1.187 0.235783 ## zipz95819 1.074e+05 5.104e+04 2.104 0.035800 * ## zipz95820 -1.712e+05 3.770e+04 -4.540 6.73e-06 *** ## zipz95821 -9.943e+04 4.344e+04 -2.289 0.022420 * ## zipz95822 -1.762e+05 4.333e+04 -4.066 5.39e-05 *** ## zipz95823 -2.001e+05 4.157e+04 -4.813 1.86e-06 *** ## zipz95824 -2.056e+05 4.097e+04 -5.019 6.77e-07 *** ## zipz95825 -1.386e+05 3.719e+04 -3.727 0.000212 *** ## zipz95826 -1.435e+05 3.811e+04 -3.765 0.000182 *** ## zipz95827 -1.880e+05 4.005e+04 -4.694 3.30e-06 *** ## zipz95828 -2.005e+05 3.935e+04 -5.096 4.60e-07 *** ## zipz95829 -1.802e+05 4.471e+04 -4.030 6.25e-05 *** ## zipz95831 -1.137e+05 5.317e+04 -2.138 0.032914 * ## zipz95832 -2.242e+05 4.891e+04 -4.584 5.52e-06 *** ## zipz95833 -1.549e+05 4.048e+04 -3.827 0.000143 *** ## zipz95834 -1.510e+05 4.150e+04 -3.639 0.000297 *** ## zipz95835 -1.202e+05 4.195e+04 -2.865 0.004309 ** ## zipz95838 -1.729e+05 3.648e+04 -4.741 2.64e-06 *** ## zipz95841 -7.757e+04 5.190e+04 -1.495 0.135538 ## zipz95842 -1.651e+05 3.921e+04 -4.210 2.93e-05 *** ## zipz95843 NA NA NA NA ## zipz95864 NA NA NA NA ## beds -1.336e+04 5.016e+03 -2.664 0.007926 ** ## baths 1.314e+04 6.323e+03 2.078 0.038099 * ## sqft 1.148e+02 7.457e+00 15.388 &lt; 2e-16 *** ## typeMulti_Family 3.034e+04 2.731e+04 1.111 0.266933 ## typeResidential 4.665e+04 1.298e+04 3.594 0.000351 *** ## latitude -2.121e+05 1.895e+05 -1.120 0.263319 ## longitude -6.519e+03 1.547e+05 -0.042 0.966399 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 4622177625) ## ## Null deviance: 1.2350e+13 on 699 degrees of freedom ## Residual deviance: 2.8981e+12 on 627 degrees of freedom ## AIC: 17635 ## ## Number of Fisher Scoring iterations: 2 # Predict on the test set pred &lt;- predict(fit, newdata = test) # Compute model performance on the test set caret::R2(pred = pred, obs = test$price) ## [1] 0.7338893 caret::RMSE(pred = pred, obs = test$price) ## [1] 66030.84 Ok, this looks much better. And we didn’t have to deal with the missing factor levels! :-) 4.6.3 h2o Authors: Tomas Nykodym, H2O.ai contributors Backend: Java The h2o package offers a data-distributed implementation of Generalized Linear Models. A “data- distribtued” version uses distributed data frames, so that the whole design matrix does not need to fit into memory at once. The h2o package fits both regularized and non-regularized GLMs. The implementation details are documented here. # h2o.glm example #install.packages(&quot;h2o&quot;) library(h2o) # Start a local H2O cluster using nthreads = num available cores h2o.init(nthreads = -1) ## ## H2O is not running yet, starting it now... ## ## Note: In case of errors look at the following log files: ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmpdbn9Q6/h2o_bradboehmke_started_from_r.out ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmpdbn9Q6/h2o_bradboehmke_started_from_r.err ## ## ## Starting H2O JVM and connecting: .. Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 2 seconds 560 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.4 ## H2O cluster version age: 28 days, 3 hours and 11 minutes ## H2O cluster name: H2O_started_from_R_bradboehmke_otb722 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 1.78 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.4.4 (2018-03-15) Typically one would load a dataset in parallel from disk using the h2o.importFile() function, however for the purposes of this tutorial, we are going to use a tiny built-in R dataset, so we can send that data to the H2O cluster (from R memory) using the as.h2o() function. We would also use the h2o.splitFrame() function to split the data instead of the caret::createDataPartition(), but for an apples-to-apples comparison with the methods above, it’s good to use the same exact train and test split, generated the same way as above. # Load Sacramento dataset library(caret) data(&quot;Sacramento&quot;) # Convert the data into an H2OFrame sac &lt;- as.h2o(Sacramento) ## | | | 0% | |=================================================================| 100% # Split the data into a 70/25% train/test sets set.seed(1) idxs &lt;- caret::createDataPartition(y = Sacramento$price, p = 0.75)[[1]] train &lt;- sac[idxs,] test &lt;- sac[-idxs,] # Dimensions dim(train) ## [1] 700 9 dim(test) ## [1] 232 9 # Columns names(train) ## [1] &quot;city&quot; &quot;zip&quot; &quot;beds&quot; &quot;baths&quot; &quot;sqft&quot; &quot;type&quot; ## [7] &quot;price&quot; &quot;latitude&quot; &quot;longitude&quot; # Identify the predictor columns xcols &lt;- setdiff(names(train), &quot;price&quot;) # Train a default GLM model with no regularization system.time( fit &lt;- h2o.glm( x = xcols, y = &quot;price&quot;, training_frame = train, family = &quot;gaussian&quot;, lambda = 0 #lambda = 0 means no regularization ) ) ## | | | 0% | |=== | 4% | |=================================================================| 100% ## user system elapsed ## 0.290 0.009 1.552 summary(fit) ## Model Details: ## ============== ## ## H2ORegressionModel: glm ## Model Key: GLM_model_R_1522981615658_1 ## GLM Model: summary ## family link regularization number_of_predictors_total ## 1 gaussian identity None 110 ## number_of_active_predictors number_of_iterations training_frame ## 1 104 1 RTMP_sid_b826_2 ## ## H2ORegressionMetrics: glm ## ** Reported on training data. ** ## ## MSE: 4140191405 ## RMSE: 64344.32 ## MAE: 46055.84 ## RMSLE: 0.2853132 ## Mean Residual Deviance : 4140191405 ## R^2 : 0.7653286 ## Null Deviance :1.234975e+13 ## Null D.o.F. :699 ## Residual Deviance :2.898134e+12 ## Residual D.o.F. :595 ## AIC :17699.32 ## ## ## ## ## ## Scoring History: ## timestamp duration iterations negative_log_likelihood ## 1 2018-04-05 22:27:00 0.000 sec 0 12349752226576.09961 ## objective ## 1 17642503180.82300 ## ## Variable Importances: (Extract with `h2o.varimp`) ## ================================================= ## ## Standardized Coefficient Magnitudes: standardized coefficient magnitudes ## names coefficients sign ## 1 zip.z95819 226542.703922 POS ## 2 zip.z95811 224933.815960 POS ## 3 zip.z95650 161683.933044 POS ## 4 city.LOOMIS 161683.933044 POS ## 5 zip.z95746 134784.591570 POS ## ## --- ## names coefficients sign ## 105 zip.z95619 0.000000 POS ## 106 zip.z95633 0.000000 POS ## 107 zip.z95663 0.000000 POS ## 108 city.DIAMOND_SPRINGS 0.000000 POS ## 109 city.GARDEN_VALLEY 0.000000 POS ## 110 city.PENRYN 0.000000 POS # H2O computes many model performance metrics automatically, accessible by utility functions perf &lt;- h2o.performance(model = fit, newdata = test) h2o.r2(perf) ## [1] 0.7377706 sqrt(h2o.mse(perf)) ## [1] 64293.34 # good practice h2o.shutdown(prompt = FALSE) ## [1] TRUE 4.6.4 speedglm Also worth metioning is the speedglm package, which fits Linear and Generalized Linear Models to large data sets. This is particularly useful if R is linked against an optimized BLAS. For data sets of size greater of R memory, the fitting is performed by an iterative algorithm. 4.7 Regularized GLM in R Ok, so let’s assume that we have wide, sparse, collinear or big data. If your training set falls into any of those categories, it might be a good idea to use a regularlized GLM. 4.7.1 glmnet Authors: Jerome Friedman, Trevor Hastie, Noah Simon, Rob Tibshirani Backend: Mortran (extension of Fortran used for scientific computation) glmnet is a package that fits a generalized linear model via penalized maximum likelihood. The regularization path is computed for the lasso or elastic-net penalty at a grid of values for the regularization parameter lambda. The algorithm is extremely fast, and can exploit sparsity in the input matrix \\(\\boldsymbol{X}\\). Features: The code can handle sparse input-matrix formats, as well as range constraints on coefficients. Glmnet also makes use of the strong rules for efficient restriction of the active set. The core of Glmnet is a set of fortran subroutines, which make for very fast execution. The algorithms use coordinate descent with warm starts and active set iterations. Supports the following distributions: &quot;gaussian&quot;,&quot;binomial&quot;,&quot;poisson&quot;,&quot;multinomial&quot;,&quot;cox&quot;,&quot;mgaussian&quot; Supports standardization and offsets. The Glmnet package is a fast implementation, but it requires some extra processing up-front to your data if it’s not already represented as a numeric matrix. For example, if you have categorical data or missing data, you need to deal with that yourself. #install.packages(&quot;glmnet&quot;) #install.packages(&quot;Cairo&quot;) #for plotting lasso coefficients in Jupyter notebook library(glmnet) data(&quot;QuickStartExample&quot;) #loads &#39;x&#39; and &#39;y&#39; str(x) ## num [1:100, 1:20] 0.274 2.245 -0.125 -0.544 -1.459 ... class(x) ## [1] &quot;matrix&quot; fit &lt;- glmnet(x, y) We can visualize the coefficients by executing the plot function. Each curve corresponds to a variable. It shows the path of its coefficient against the \\(\\ell_1\\)-norm of the whole coefficient vector at as \\(\\lambda\\) varies. The axis above indicates the number of nonzero coefficients at the current \\(\\lambda\\), which is the effective degrees of freedom for the lasso. plot(fit) # Simulate a binary response dataset library(caret) set.seed(1) df &lt;- caret::twoClassSim( n = 100000, linearVars = 10, noiseVars = 50, corrVars = 50 ) dim(df) ## [1] 100000 116 # Identify the response &amp; predictor columns ycol &lt;- &quot;Class&quot; xcols &lt;- setdiff(names(df), ycol) df[,ycol] &lt;- ifelse(df[,ycol]==&quot;Class1&quot;, 0, 1) # Split the data into a 70/25% train/test sets set.seed(1) idxs &lt;- caret::createDataPartition(y = df[,ycol], p = 0.75)[[1]] train &lt;- df[idxs,] test &lt;- df[-idxs,] train_y &lt;- df[idxs, ycol] test_y &lt;- df[-idxs, ycol] train_x &lt;- model.matrix(~-1 + ., train[, xcols]) test_x &lt;- model.matrix(~-1 + ., test[, xcols]) # Dimensions dim(train_x) ## [1] 75000 115 length(train_y) ## [1] 75000 dim(test_x) ## [1] 25000 115 length(test_y) ## [1] 25000 head(test_y) ## [1] 0 1 1 0 0 0 # Train a Lasso GLM system.time( cvfit &lt;- cv.glmnet( x = train_x, y = train_y, family = &quot;binomial&quot;, alpha = 1.0 # alpha = 1 means lasso by default ) ) ## user system elapsed ## 56.088 1.165 57.369 preds &lt;- predict( cvfit$glmnet.fit, newx = test_x, s = cvfit$lambda.min, type = &quot;response&quot; ) head(preds) ## 1 ## 2 0.3136236 ## 14 0.6544411 ## 15 0.9269314 ## 21 0.1027764 ## 29 0.6640567 ## 30 0.5079524 #install.packages(&quot;cvAUC&quot;) library(cvAUC) cvAUC::AUC(predictions = preds, labels = test_y) ## [1] 0.9082888 4.7.2 h2o Introduced in the previous section, the h2o package can perform unregularized or regularized regression. By default, h2o.glm will perform an Elastic Net regression. Similar to the glmnet function, you can adjust the Elastic Net penalty through the alpha parameter (alpha = 1.0 is Lasso and alpha = 0.0 is Ridge). # Simulate a binary response dataset library(caret) set.seed(1) df &lt;- caret::twoClassSim( n = 100000, linearVars = 10, noiseVars = 50, corrVars = 50 ) dim(df) ## [1] 100000 116 # Convert the data into an H2OFrame library(h2o) # initialize h2o instance h2o.init(nthreads = -1) ## ## H2O is not running yet, starting it now... ## ## Note: In case of errors look at the following log files: ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmpdbn9Q6/h2o_bradboehmke_started_from_r.out ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmpdbn9Q6/h2o_bradboehmke_started_from_r.err ## ## ## Starting H2O JVM and connecting: ... Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 2 seconds 823 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.4 ## H2O cluster version age: 28 days, 3 hours and 12 minutes ## H2O cluster name: H2O_started_from_R_bradboehmke_qca880 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 1.78 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.4.4 (2018-03-15) # Convert the data into an H2OFrame hf &lt;- as.h2o(df) ## | | | 0% | |=================================================================| 100% # Identify the response &amp; predictor columns ycol &lt;- &quot;Class&quot; xcols &lt;- setdiff(names(hf), ycol) # Convert the 0/1 binary response to a factor hf[,ycol] &lt;- as.factor(hf[,ycol]) dim(hf) ## [1] 100000 116 # Split the data into a 70/25% train/test sets set.seed(1) idxs &lt;- caret::createDataPartition(y = df[,ycol], p = 0.75)[[1]] train &lt;- hf[idxs,] test &lt;- hf[-idxs,] # Dimensions dim(train) ## [1] 75001 116 dim(test) ## [1] 24999 116 # Train a Lasso GLM system.time( fit &lt;- h2o.glm( x = xcols, y = ycol, training_frame = train, family = &quot;binomial&quot;, lambda_search = TRUE, # compute lasso path alpha = 1 # alpha = 1 means lasso, same as glmnet above ) ) ## | | | 0% | |=== | 4% | |============= | 20% | |======================= | 36% | |================================ | 49% | |=================================== | 54% | |====================================== | 58% | |======================================== | 61% | |=================================================================| 100% ## user system elapsed ## 0.449 0.013 8.846 # Compute AUC on test dataset # H2O computes many model performance metrics automatically, including AUC perf &lt;- h2o.performance( model = fit, newdata = test ) h2o.auc(perf) ## [1] 0.911921 # good practice h2o.shutdown(prompt = FALSE) ## [1] TRUE 4.8 References [1] [https://en.wikipedia.org/wiki/Linear_regression#Generalized\\_linear\\_models](https://en.wikipedia.org/wiki/Linear_regression#Generalized_linear_models) [2] [https://en.wikipedia.org/wiki/Generalized\\_linear\\_model](https://en.wikipedia.org/wiki/Generalized_linear_model) [3] [Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. J.Royal. Statist. Soc B., Vol. 58, No. 1, pages 267-288).](http://www-stat.stanford.edu/%7Etibs/lasso/lasso.pdf) "],
["deep-neural-networks.html", "Chapter 5 Deep Neural Networks (DNN) 5.1 Introduction 5.2 History 5.3 Backpropagation 5.4 Architectures 5.5 Visualizing Neural Nets 5.6 Deep Learning Software in R 5.7 References", " Chapter 5 Deep Neural Networks (DNN) Image Source: https://machprinciple.wordpress.com/2013/11/09/neural-network-controlled-spacecraft-control-logic-exists/ 5.1 Introduction A Deep Neural Network (DNN) is an artificial neural network (ANN) with multiple hidden layers of units between the input and output layers. Similar to shallow ANNs, DNNs can model complex non-linear relationships. DNN architectures (e.g. for object detection and parsing) generate compositional models where the object is expressed as a layered composition of image primitives. The extra layers enable composition of features from lower layers, giving the potential of modeling complex data with fewer units than a similarly performing shallow network. DNNs are typically designed as feedforward networks, but research has very successfully applied recurrent neural networks, especially LSTM, for applications such as language modeling. Convolutional deep neural networks (CNNs) are used in computer vision where their success is well- documented. CNNs also have been applied to acoustic modeling for automatic speech recognition, where they have shown success over previous models. This tutorial will cover DNNs, however, we will introduce the concept of a “shallow” neural network as a starting point. 5.2 History An interesting fact about neural networks is that the first artificial neural network was implemented in hardware, not software. In 1943, neurophysiologist Warren McCulloch and mathematician Walter Pitts wrote a paper on how neurons might work. In order to describe how neurons in the brain might work, they modeled a simple neural network using electrical circuits. [1] 5.3 Backpropagation Backpropagation, an abbreviation for “backward propagation of errors”, is a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of a loss function with respect to all the weights in the network. The gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the loss function. 5.4 Architectures 5.4.1 Multilayer Perceptron (MLP) A multilayer perceptron (MLP) is a feed- forward artificial neural network model that maps sets of input data onto a set of appropriate outputs. This is also called a fully-connected fFeed-forward ANN. An MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training the network. MLP is a modification of the standard linear perceptron and can distinguish data that are not linearly separable. Image Souce: neuralnetworksanddeeplearning.com 5.4.2 Recurrent A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feed-forward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. This makes them applicable to tasks such as unsegmented connected handwriting recognition or speech recognition. There are a number of Twitter bots that are created using LSTMs. Some fun examples are (???)(https://twitter.com/DeepDrumpf) and (???)(https://twitter.com/DeepLearnBern) by Brad Hayes from MIT. 5.4.3 Convolutional A convolutional neural network (CNN, or ConvNet) is a type of feed- forward artificial neural network in which the connectivity pattern between its neurons is inspired by the organization of the animal visual cortex, whose individual neurons are arranged in such a way that they respond to overlapping regions tiling the visual field. Convolutional networks were inspired by biological processes and are variations of multilayer perceptrons designed to use minimal amounts of preprocessing. They have wide applications in image and video recognition, recommender systems and natural language processing. Some creative application of CNNs are DeepDream images and Neural Style Transfer. 5.5 Visualizing Neural Nets http://playground.tensorflow.org is a website where you can tweak and visualize neural networks. 5.6 Deep Learning Software in R For the purposes of this tutorial, we will review CPU-based deep learning packages in R that support numeric, tabular data (data frames). There is deep learning software that supports image data directly, but we will not cover those features in this tutorial. The demos below will train a fully-connected, feed- forward ANN (aka mutilayer perceptron) using the mxnet and h2o R packages. Also worth noting are two other Deep Learning packages in R, deepnet and darch (which I don’t have much experience with). 5.6.1 MXNet Authors: Tianqi Chen, Qiang Kou (KK), et. al. Backend: C++ MXNet is deep neural net implementation by the same authors as xgboost, and a part of the same umbrella project called DMLC (Distributed Machine Learning Community). The MXNet R package brings flexible and efficient GPU computing and state-of-art deep learning to R. Features: It enables you to write seamless tensor/matrix computation with multiple GPUs in R. Distributed computation. Read image files directly. Offers both a simplified and complex interface for architecting networks. Supports mulit-layer feed-forward ANNs (aka. multi-layer perceptrons (MLP)). Supports Convolutional Neural Networks (CNN) – good for image recognition. Supports Long Short-term memory (LSTM), a type of Recurrent Neural Network (RNN) – good for sequence learning, (e.g. speech, text). License: BSD MXNet Example code below modified from: http://www.r-bloggers.com/deep-learning- with-mxnetr/ The example below contains the canonical 60k/10k MNIST train/test files as opposed to the Kaggle version in the blog post. # Install pre-build CPU-based mxnet package (no GPU support) # install.packages(&quot;drat&quot;) # drat:::addRepo(&quot;dmlc&quot;) # cran &lt;- getOption(&quot;repos&quot;) # cran[&quot;dmlc&quot;] &lt;- &quot;https://s3-us-west-2.amazonaws.com/apache-mxnet/R/CRAN/&quot; # options(repos = cran) # install.packages(&quot;mxnet&quot;,dependencies = T) library(mxnet) # Data load &amp; preprocessing ## load training data https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz train &lt;- data.table::fread(&quot;data/mnist_train.csv&quot;) ## Read 66.7% of 60000 rows Read 60000 rows and 785 (of 785) columns from 0.102 GB file in 00:00:03 ## load test data https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz test &lt;- data.table::fread(&quot;data/mnist_test.csv&quot;) train &lt;- data.matrix(train) test &lt;- data.matrix(test) train &lt;- data.matrix(train) test &lt;- data.matrix(test) train_x &lt;- train[,-785] train_y &lt;- train[,785] test_x &lt;- test[,-785] test_y &lt;- test[,785] # lineraly transform it into [0,1] # transpose input matrix to npixel x nexamples (format expected by mxnet) train_x &lt;- t(train_x/255) test_x &lt;- t(test_x/255) #see that the number of each digit is fairly even table(train_y) ## train_y ## 0 1 2 3 4 5 6 7 8 9 ## 5923 6742 5958 6131 5842 5421 5918 6265 5851 5949 # Configure the structure of the network # in mxnet use its own data type symbol to configure the network data &lt;- mx.symbol.Variable(&quot;data&quot;) # set the first hidden layer where data is the input, name and number of hidden neurons fc1 &lt;- mx.symbol.FullyConnected(data, name = &quot;fc1&quot;, num_hidden = 128) # set the activation which takes the output from the first hidden layer fc1 act1 &lt;- mx.symbol.Activation(fc1, name = &quot;relu1&quot;, act_type = &quot;relu&quot;) # second hidden layer takes the result from act1 as the input, name and number of hidden neurons fc2 &lt;- mx.symbol.FullyConnected(act1, name = &quot;fc2&quot;, num_hidden = 64) # second activation which takes the output from the second hidden layer fc2 act2 &lt;- mx.symbol.Activation(fc2, name = &quot;relu2&quot;, act_type = &quot;relu&quot;) # this is the output layer where number of nuerons is set to 10 because there&#39;s only 10 digits fc3 &lt;- mx.symbol.FullyConnected(act2, name = &quot;fc3&quot;, num_hidden = 10) # finally set the activation to softmax to get a probabilistic prediction softmax &lt;- mx.symbol.SoftmaxOutput(fc3, name = &quot;sm&quot;) # set which device to use before we start the computation # assign cpu to mxnet devices &lt;- mx.cpu() # set seed to control the random process in mxnet mx.set.seed(0) # train the nueral network model &lt;- mx.model.FeedForward.create( softmax, X = train_x, y = train_y, ctx = devices, num.round = 10, array.batch.size = 100, learning.rate = 0.07, momentum = 0.9, eval.metric = mx.metric.accuracy, initializer = mx.init.uniform(0.07), epoch.end.callback = mx.callback.log.train.metric(100) ) # make a prediction preds &lt;- predict(model, test_x) dim(preds) ## [1] 10 10000 ## [1] 10 10000 # matrix with 10000 rows and 10 columns containing the classification probabilities from the output layer # use max.col to extract the maximum label for each row pred_label &lt;- max.col(t(preds)) - 1 table(pred_label) ## pred_label ## 0 1 2 3 4 5 6 7 8 9 ## 1004 1140 1018 1029 929 880 946 1022 981 1051 # Compute accuracy acc &lt;- sum(test_y == pred_label)/length(test_y) print(acc) ## [1] 0.977 MXNet also has a simplified api, mx.mlp(), which provides a more generic interface compared to the network construction procedure above. 5.6.2 h2o Authors: Arno Candel, H2O.ai contributors Backend: Java H2O Deep Learning builds a feed-forward multilayer artificial neural network on an distributed H2O data frame. Features: Distributed and parallelized computation on either a single node or a multi- node cluster. Data-distributed, which means the entire dataset does not need to fit into memory on a single node. Uses HOGWILD! for fast computation. Simplified network building interface. Fully connected, feed-forward ANNs only (CNNs and LSTMs in development). CPU only (GPU interface in development). Automatic early stopping based on convergence of user-specied metrics to user- specied relative tolerance. Support for exponential families (Poisson, Gamma, Tweedie) and loss functions in addition to binomial (Bernoulli), Gaussian and multinomial distributions, such as Quantile regression (including Laplace). Grid search for hyperparameter optimization and model selection. Apache 2.0 Licensed. Model export in plain Java code for deployment in production environments. GUI for training &amp; model eval/viz (H2O Flow). library(h2o) h2o.init(nthreads = -1) # This means nthreads = num available cores ## ## H2O is not running yet, starting it now... ## ## Note: In case of errors look at the following log files: ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmpo3M4DD/h2o_bradboehmke_started_from_r.out ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmpo3M4DD/h2o_bradboehmke_started_from_r.err ## ## ## Starting H2O JVM and connecting: .. Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 2 seconds 643 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.4 ## H2O cluster version age: 28 days, 3 hours and 13 minutes ## H2O cluster name: H2O_started_from_R_bradboehmke_ygp041 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 1.78 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.4.4 (2018-03-15) # Data load &amp; preprocessing train &lt;- h2o.importFile(&quot;data/mnist_train.csv&quot;) ## | | | 0% | |================================================= | 75% | |=================================================================| 100% test &lt;- h2o.importFile(&quot;data/mnist_test.csv&quot;) ## | | | 0% | |================ | 25% | |=================================================================| 100% # Specify the response and predictor columns y &lt;- &quot;C785&quot; x &lt;- setdiff(names(train), y) # We encode the response column as categorical for multinomial classification train[,y] &lt;- as.factor(train[,y]) test[,y] &lt;- as.factor(test[,y]) # Train an H2O Deep Learning model model &lt;- h2o.deeplearning( x = x, y = y, training_frame = train, sparse = TRUE, #speed-up on sparse data (like MNIST) distribution = &quot;multinomial&quot;, activation = &quot;Rectifier&quot;, hidden = c(128,64), epochs = 10 ) ## | | | 0% | |= | 2% | |== | 3% | |=== | 5% | |==== | 7% | |====== | 8% | |======= | 10% | |======== | 12% | |========= | 14% | |========== | 15% | |=========== | 17% | |============ | 19% | |============= | 20% | |============== | 22% | |=============== | 24% | |================= | 25% | |================== | 27% | |=================== | 29% | |==================== | 31% | |===================== | 32% | |====================== | 34% | |======================= | 36% | |======================== | 37% | |========================= | 39% | |========================== | 41% | |============================ | 42% | |============================= | 44% | |============================== | 46% | |=============================== | 48% | |================================ | 49% | |================================= | 51% | |================================== | 53% | |=================================== | 54% | |==================================== | 56% | |====================================== | 58% | |======================================= | 59% | |======================================== | 61% | |========================================= | 63% | |========================================== | 65% | |=========================================== | 66% | |============================================ | 68% | |============================================= | 70% | |============================================== | 71% | |=============================================== | 73% | |================================================= | 75% | |================================================== | 76% | |=================================================== | 78% | |==================================================== | 80% | |===================================================== | 82% | |====================================================== | 83% | |======================================================= | 85% | |======================================================== | 87% | |========================================================= | 88% | |=========================================================== | 90% | |============================================================ | 92% | |============================================================= | 93% | |============================================================== | 95% | |=============================================================== | 97% | |================================================================ | 99% | |=================================================================| 100% # Get model performance on a test set perf &lt;- h2o.performance(model, test) # Or get the preds directly and compute classification accuracy preds &lt;- h2o.predict(model, newdata = test) ## | | | 0% | |=================================================================| 100% acc &lt;- sum(test[,y] == preds$predict)/nrow(test) print(acc) ## [1] 0.9724 # good practice h2o.shutdown(prompt = FALSE) ## [1] TRUE 5.7 References [1] [http://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html](http://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html) "],
["stacking.html", "Chapter 6 Stacking 6.1 Introduction 6.2 Background 6.3 Common Types of Ensemble Methods 6.4 The Super Learner Algorithm 6.5 Stacking Software in R", " Chapter 6 Stacking Figure .: Super Learner. 6.1 Introduction Stacking (sometimes called “stacked generalization”) involves training a learning algorithm to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm, the metalearner, is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. If an arbitrary metalearning algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although in practice, a single-layer logistic regression model is often used for metalearning. Stacking typically yields performance better than any single one of the trained models in the ensemble. 6.2 Background Leo Breiman, known for his work on classification and regression trees and the creator of the Random Forest algorithm, formalized stacking in his 1996 paper, “Stacked Regressions”. Although the idea originated with David Wolpert in 1992 under the name “Stacked Generalization”, the modern form of stacking that uses internal k-fold cross-validation was Dr. Breiman’s contribution. However, it wasn’t until 2007 that the theoretical background for stacking was developed, which is when the algorithm took on the name, “Super Learner”. Until this time, the mathematical reasons for why stacking worked were unknown and stacking was considered a “black art.” The Super Learner algorithm learns the optimal combination of the base learner fits. In an article titled, “Super Learner”, by Mark van der Laan et al., proved that the Super Learner ensemble represents an asymptotically optimal system for learning. 6.3 Common Types of Ensemble Methods In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained by any of the constituent algorithms. 6.3.1 Bagging Reduces variance and increases accuracy Robust against outliers or noisy data Often used with Decision Trees (i.e. Random Forest) 6.3.2 Boosting Also reduces variance and increases accuracy Not robust against outliers or noisy data Flexible - can be used with any loss function 6.3.3 Stacking Used to ensemble a diverse group of strong learners Involves training a second-level machine learning algorithm called a “metalearner” to learn the optimal combination of the base learners 6.4 The Super Learner Algorithm A common task in machine learning is to perform model selection by specifying a number of models with different parameters. An example of this is Grid Search. The first phase of the Super Learner algorithm is computationally equivalent to performing model selection via cross-validation. The latter phase of the Super Learner algorithm (the metalearning step) is just training another single model (no cross-validation) on the level one data. 6.4.1 1. Set up the ensemble Specify a list of \\(L\\) base algorithms (with a specific set of model parameters). These are also called base learners. Specify a metalearning algorithm (just another algorithm). 6.4.2 2. Train the ensemble 6.4.2.1 Cross-validate Base Learners Perform k-fold cross-validation on each of these learners and collect the cross-validated predicted values from each of the \\(L\\) algorithms. The \\(N\\) cross-validated predicted values from each of the \\(L\\) algorithms can be combined to form a new \\(N \\times L\\) matrix. This matrix, along wtih the original response vector, is called the “level-one” data. Figure 1.1: Stacking. 6.4.2.2 Metalearning Train the metalearning algorithm on the level-one data. Train each of the \\(L\\) base algorithms on the full training set. The “ensemble model” consists of the \\(L\\) base learning models and the metalearning model, which can then be used to generate predictions on a test set. 6.4.3 3. Predict on new data To generate ensemble predictions, first generate predictions from the base learners. Feed those predictions into the metalearner model to generate the ensemble prediction. 6.5 Stacking Software in R Stacking is a broad class of algorithms that involves training a second-level “metalearner” to ensemble a group of base learners. The three packages in the R ecosystem which implement the Super Learner algorithm (stacking on cross-validated predictions) are SuperLearner, subsemble and h2oEnsemble. Among ensemble software in R, there is also caretEnsemble, but it implements a boostrapped (rather than cross-validated) version of stacking via the caretStack() function. The bootstrapped version will train faster since bootrapping (with a train/test) is a fraction of the work as k-fold cross-validation, however the the ensemble performance suffers as a result of this shortcut. 6.5.1 SuperLearner Authors: Eric Polley, Erin LeDell, Mark van der Laan Backend: R with constituent algorithms written in a variety of languages The original Super Learner implemenation is the SuperLearner R package (2010). Features: Implements the Super Learner prediction method (stacking) and contains a library of prediction algorithms to be used in the Super Learner. Provides a clean interface to 30+ algorithms in R and defines a consistent API for extensibility. GPL-3 Licensed. 6.5.2 subsemble Authors: Erin LeDell, Stephanie Sapp Backend: R with constituent algorithms written in a variety of languages Subsemble is a general subset ensemble prediction method, which can be used for small, moderate, or large datasets. Subsemble partitions the full dataset into subsets of observations, fits a specified underlying algorithm on each subset, and uses a unique form of k-fold cross-validation to output a prediction function that combines the subset-specific fits. An oracle result provides a theoretical performance guarantee for Subsemble. Features: Implements the Subsemble Algorithm. Implements the Super Learner Algorithm (stacking). Uses the SuperLearner wrapper interface for defining base learners and metalearners. Multicore and multi-node cluster support via the snow R package. Improved parallelization over the SuperLearner package. Apache 2.0 Licensed. 6.5.3 H2O Ensemble Authors: Erin LeDell Backend: Java H2O Ensemble has been implemented as a stand-alone R package called h2oEnsemble. The package is an extension to the h2o R package that allows the user to train an ensemble in the H2O cluster using any of the supervised machine learning algorithms H2O. Features: Uses data-distributed and parallelized Java-based algorithms for the ensemble. All training and data processing are performed in the high-performance H2O cluster rather than in R memory. Supports regression and binary classification. Multi-class support in development. Code refactor underway (moving from a separate R package into H2O “proper”) so that the H2O Ensemble interface can be exposed in Python and Scala. Apache 2.0 Licensed. # Install h2oEnsemble from GitHub install.packages(&quot;https://h2o-release.s3.amazonaws.com/h2o-ensemble/R/h2oEnsemble_0.2.1.tar.gz&quot;, repos = NULL) library(h2oEnsemble) 6.5.4 Higgs Demo This is an example of binary classification using the h2o.ensemble function, which is available in h2oEnsemble. This demo uses a subset of the HIGGS dataset, which has 28 numeric features and a binary response. The machine learning task in this example is to distinguish between a signal process which produces Higgs bosons (Y = 1) and a background process which does not (Y = 0). The dataset contains approximately the same number of positive vs negative examples. In other words, this is a balanced, rather than imbalanced, dataset. If run from plain R, execute R in the directory of this script. If run from RStudio, be sure to setwd() to the location of this script. h2o.init() starts H2O in R’s current working directory. h2o.importFile() looks for files from the perspective of where H2O was started. 6.5.4.1 Start H2O Cluster h2o.init(nthreads = -1) # Start an H2O cluster with nthreads = num cores on your machine ## ## H2O is not running yet, starting it now... ## ## Note: In case of errors look at the following log files: ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//RtmpLAu9RE/h2o_bradboehmke_started_from_r.out ## /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//RtmpLAu9RE/h2o_bradboehmke_started_from_r.err ## ## ## Starting H2O JVM and connecting: .. Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 2 seconds 526 milliseconds ## H2O cluster timezone: America/New_York ## H2O data parsing timezone: UTC ## H2O cluster version: 3.18.0.4 ## H2O cluster version age: 28 days, 3 hours and 15 minutes ## H2O cluster name: H2O_started_from_R_bradboehmke_kdm557 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 1.78 GB ## H2O cluster total cores: 4 ## H2O cluster allowed cores: 4 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## H2O API Extensions: XGBoost, Algos, AutoML, Core V3, Core V4 ## R Version: R version 3.4.4 (2018-03-15) h2o.removeAll() # (Optional) Remove all objects in H2O cluster ## [1] 0 6.5.4.2 Load Data into H2O Cluster First, import a sample binary outcome train and test set into the H2O cluster. # import data train &lt;- data.table::fread(&quot;https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv&quot;) test &lt;- data.table::fread(&quot;https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv&quot;) # convert to h2o objects train &lt;- as.h2o(train) test &lt;- as.h2o(test) y &lt;- &quot;response&quot; x &lt;- setdiff(names(train), y) family &lt;- &quot;binomial&quot; For binary classification, the response should be encoded as a factor type (also known as the enum type in Java or categorial in Python Pandas). The user can specify column types in the h2o.importFile command, or you can convert the response column as follows: train[, y] &lt;- as.factor(train[, y]) test[, y] &lt;- as.factor(test[, y]) 6.5.4.3 Specify Base Learners &amp; Metalearner For this example, we will use the default base learner library for h2o.ensemble, which includes the default H2O GLM, Random Forest, GBM and Deep Neural Net (all using default model parameter values). We will also use the default metalearner, the H2O GLM. learner &lt;- c(&quot;h2o.glm.wrapper&quot;, &quot;h2o.randomForest.wrapper&quot;, &quot;h2o.gbm.wrapper&quot;, &quot;h2o.deeplearning.wrapper&quot;) metalearner &lt;- &quot;h2o.glm.wrapper&quot; 6.5.4.4 Train an Ensemble Train the ensemble (using 5-fold internal CV) to generate the level-one data. Note that more CV folds will take longer to train, but should increase performance. fit &lt;- h2o.ensemble( x = x, y = y, training_frame = train, family = family, learner = learner, metalearner = metalearner, cvControl = list(V = 5) ) 6.5.4.5 Evaluate Model Performance Since the response is binomial, we can use Area Under the ROC Curve (AUC) to evaluate the model performance. Compute test set performance, and sort by AUC (the default metric that is printed for a binomial classification): perf &lt;- h2o.ensemble_performance(fit, newdata = test) Print the base learner and ensemble performance: perf ## ## Base learner performance, sorted by specified metric: ## learner AUC ## 1 h2o.glm.wrapper 0.6870611 ## 4 h2o.deeplearning.wrapper 0.7524101 ## 2 h2o.randomForest.wrapper 0.7686478 ## 3 h2o.gbm.wrapper 0.7817084 ## ## ## H2O Ensemble Performance on &lt;newdata&gt;: ## ---------------- ## Family: binomial ## ## Ensemble performance (AUC): 0.788077753779698 We can compare the performance of the ensemble to the performance of the individual learners in the ensemble. So we see the best individual algorithm in this group is the GBM with a test set AUC of 0.778, as compared to 0.781 for the ensemble. At first thought, this might not seem like much, but in many industries like medicine or finance, this small advantage can be highly valuable. To increase the performance of the ensemble, we have several options. One of them is to increase the number of internal cross-validation folds using the cvControl argument. The other options are to change the base learner library or the metalearning algorithm. Note that the ensemble results above are not reproducible since h2o.deeplearning is not reproducible when using multiple cores, and we did not set a seed for h2o.randomForest.wrapper. If we want to evaluate the model by a different metric, say “MSE”, then we can pass that metric to the print method for and ensemble performance object as follows: print(perf, metric = &quot;MSE&quot;) ## ## Base learner performance, sorted by specified metric: ## learner MSE ## 1 h2o.glm.wrapper 0.2217110 ## 4 h2o.deeplearning.wrapper 0.2055667 ## 2 h2o.randomForest.wrapper 0.1972701 ## 3 h2o.gbm.wrapper 0.1900488 ## ## ## H2O Ensemble Performance on &lt;newdata&gt;: ## ---------------- ## Family: binomial ## ## Ensemble performance (MSE): 0.187181902317254 6.5.4.6 Predict If you actually need to generate the predictions (instead of looking only at model performance), you can use the predict() function with a test set. Generate predictions on the test set and store as an H2O Frame: pred &lt;- predict(fit, newdata = test) If you need to bring the predictions back into R memory for futher processing, you can convert pred to a local R data.frame as follows: predictions &lt;- as.data.frame(pred$pred)[,3] #third column is P(Y==1) labels &lt;- as.data.frame(test[,y])[,1] The predict method for an h2o.ensemble fit will return a list of two objects. The pred$pred object contains the ensemble predictions, and pred$basepred is a matrix of predictions from each of the base learners. In this particular example where we used four base learners, the pred$basepred matrix has four columns. Keeping the base learner predictions around is useful for model inspection and will allow us to calculate performance of each of the base learners on the test set (for comparison to the ensemble). 6.5.4.7 Specifying new learners Now let’s try again with a more extensive set of base learners. The h2oEnsemble packages comes with four functions by default that can be customized to use non-default parameters. Here is an example of how to generate a custom learner wrappers: h2o.glm.1 &lt;- function(..., alpha = 0.0) h2o.glm.wrapper(..., alpha = alpha) h2o.glm.2 &lt;- function(..., alpha = 0.5) h2o.glm.wrapper(..., alpha = alpha) h2o.glm.3 &lt;- function(..., alpha = 1.0) h2o.glm.wrapper(..., alpha = alpha) h2o.randomForest.1 &lt;- function(..., ntrees = 200, nbins = 50, seed = 1) h2o.randomForest.wrapper(..., ntrees = ntrees, nbins = nbins, seed = seed) h2o.randomForest.2 &lt;- function(..., ntrees = 200, sample_rate = 0.75, seed = 1) h2o.randomForest.wrapper(..., ntrees = ntrees, sample_rate = sample_rate, seed = seed) h2o.randomForest.3 &lt;- function(..., ntrees = 200, sample_rate = 0.85, seed = 1) h2o.randomForest.wrapper(..., ntrees = ntrees, sample_rate = sample_rate, seed = seed) h2o.randomForest.4 &lt;- function(..., ntrees = 200, nbins = 50, balance_classes = TRUE, seed = 1) h2o.randomForest.wrapper(..., ntrees = ntrees, nbins = nbins, balance_classes = balance_classes, seed = seed) h2o.gbm.1 &lt;- function(..., ntrees = 100, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, seed = seed) h2o.gbm.2 &lt;- function(..., ntrees = 100, nbins = 50, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, nbins = nbins, seed = seed) h2o.gbm.3 &lt;- function(..., ntrees = 100, max_depth = 10, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, max_depth = max_depth, seed = seed) h2o.gbm.4 &lt;- function(..., ntrees = 100, col_sample_rate = 0.8, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, col_sample_rate = col_sample_rate, seed = seed) h2o.gbm.5 &lt;- function(..., ntrees = 100, col_sample_rate = 0.7, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, col_sample_rate = col_sample_rate, seed = seed) h2o.gbm.6 &lt;- function(..., ntrees = 100, col_sample_rate = 0.6, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, col_sample_rate = col_sample_rate, seed = seed) h2o.gbm.7 &lt;- function(..., ntrees = 100, balance_classes = TRUE, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, balance_classes = balance_classes, seed = seed) h2o.gbm.8 &lt;- function(..., ntrees = 100, max_depth = 3, seed = 1) h2o.gbm.wrapper(..., ntrees = ntrees, max_depth = max_depth, seed = seed) h2o.deeplearning.1 &lt;- function(..., hidden = c(500,500), activation = &quot;Rectifier&quot;, epochs = 50, seed = 1) h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed) h2o.deeplearning.2 &lt;- function(..., hidden = c(200,200,200), activation = &quot;Tanh&quot;, epochs = 50, seed = 1) h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed) h2o.deeplearning.3 &lt;- function(..., hidden = c(500,500), activation = &quot;RectifierWithDropout&quot;, epochs = 50, seed = 1) h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed) h2o.deeplearning.4 &lt;- function(..., hidden = c(500,500), activation = &quot;Rectifier&quot;, epochs = 50, balance_classes = TRUE, seed = 1) h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, balance_classes = balance_classes, seed = seed) h2o.deeplearning.5 &lt;- function(..., hidden = c(100,100,100), activation = &quot;Rectifier&quot;, epochs = 50, seed = 1) h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed) h2o.deeplearning.6 &lt;- function(..., hidden = c(50,50), activation = &quot;Rectifier&quot;, epochs = 50, seed = 1) h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed) h2o.deeplearning.7 &lt;- function(..., hidden = c(100,100), activation = &quot;Rectifier&quot;, epochs = 50, seed = 1) h2o.deeplearning.wrapper(..., hidden = hidden, activation = activation, seed = seed) Let’s grab a subset of these learners for our base learner library and re-train the ensemble. 6.5.4.8 Customized base learner library learner &lt;- c( &quot;h2o.glm.wrapper&quot;, &quot;h2o.randomForest.1&quot;, &quot;h2o.randomForest.2&quot;, &quot;h2o.gbm.1&quot;, &quot;h2o.gbm.6&quot;, &quot;h2o.gbm.8&quot;, &quot;h2o.deeplearning.1&quot;, &quot;h2o.deeplearning.6&quot;, &quot;h2o.deeplearning.7&quot; ) Train with new library: fit &lt;- h2o.ensemble( x = x, y = y, training_frame = train, family = family, learner = learner, metalearner = metalearner, cvControl = list(V = 5) ) Evaluate the test set performance: perf &lt;- h2o.ensemble_performance(fit, newdata = test) We see an increase in performance by including a more diverse library. Base learner test AUC (for comparison) perf ## ## Base learner performance, sorted by specified metric: ## learner AUC ## 1 h2o.glm.wrapper 0.6870611 ## 8 h2o.deeplearning.6 0.7250912 ## 7 h2o.deeplearning.1 0.7304767 ## 9 h2o.deeplearning.7 0.7422359 ## 2 h2o.randomForest.1 0.7798627 ## 6 h2o.gbm.8 0.7804816 ## 4 h2o.gbm.1 0.7805141 ## 3 h2o.randomForest.2 0.7821736 ## 5 h2o.gbm.6 0.7825354 ## ## ## H2O Ensemble Performance on &lt;newdata&gt;: ## ---------------- ## Family: binomial ## ## Ensemble performance (AUC): 0.789971483845538 So what happens to the ensemble if we remove some of the weaker learners? Let’s remove the GLM and DL from the learner library and see what happens… Here is a more stripped down version of the base learner library used above: learner &lt;- c( &quot;h2o.randomForest.1&quot;, &quot;h2o.randomForest.2&quot;, &quot;h2o.gbm.1&quot;, &quot;h2o.gbm.6&quot;, &quot;h2o.gbm.8&quot; ) Again re-train the ensemble and evaluate the performance: fit &lt;- h2o.ensemble( x = x, y = y, training_frame = train, family = family, learner = learner, metalearner = metalearner, cvControl = list(V = 5) ) perf &lt;- h2o.ensemble_performance(fit, newdata = test) We actually lose ensemble performance by removing the weak learners! This demonstrates the power of stacking with a large and diverse set of base learners. perf ## ## Base learner performance, sorted by specified metric: ## learner AUC ## 1 h2o.randomForest.1 0.7798627 ## 5 h2o.gbm.8 0.7804816 ## 3 h2o.gbm.1 0.7805141 ## 2 h2o.randomForest.2 0.7821736 ## 4 h2o.gbm.6 0.7825354 ## ## ## H2O Ensemble Performance on &lt;newdata&gt;: ## ---------------- ## Family: binomial ## ## Ensemble performance (AUC): 0.787453857322699 At first thought, you may assume that removing less performant models would increase the perforamnce of the ensemble. However, each learner has it’s own unique contribution to the ensemble and the added diversity among learners usually improves performance. The Super Learner algorithm learns the optimal way of combining all these learners together in a way that is superior to other combination/blending methods. 6.5.5 Stacking Existing Model Sets You can also use an existing (cross-validated) list of H2O models as the starting point and use the h2o.stack() function to ensemble them together via a specified metalearner. The base models must have been trained on the same dataset with same response and for cross-validation, must have all used the same folds. An example follows. As above, start up the H2O cluster and load the training and test data. Cross-validate and train a handful of base learners and then use the h2o.stack() function to create the ensemble: # The h2o.stack function is an alternative to the h2o.ensemble function, which # allows the user to specify H2O models individually and then stack them together # at a later time. Saved models, re-loaded from disk, can also be stacked. # The base models must use identical cv folds; this can be achieved in two ways: # 1. they be specified explicitly by using the fold_column argument, or # 2. use same value for `nfolds` and set `fold_assignment = &quot;Modulo&quot;` nfolds &lt;- 5 glm1 &lt;- h2o.glm( x = x, y = y, family = family, training_frame = train, nfolds = nfolds, fold_assignment = &quot;Modulo&quot;, keep_cross_validation_predictions = TRUE ) gbm1 &lt;- h2o.gbm( x = x, y = y, distribution = &quot;bernoulli&quot;, training_frame = train, seed = 1, nfolds = nfolds, fold_assignment = &quot;Modulo&quot;, keep_cross_validation_predictions = TRUE ) rf1 &lt;- h2o.randomForest( x = x, y = y, # distribution not used for RF training_frame = train, seed = 1, nfolds = nfolds, fold_assignment = &quot;Modulo&quot;, keep_cross_validation_predictions = TRUE ) dl1 &lt;- h2o.deeplearning( x = x, y = y, distribution = &quot;bernoulli&quot;, training_frame = train, nfolds = nfolds, fold_assignment = &quot;Modulo&quot;, keep_cross_validation_predictions = TRUE ) models &lt;- list(glm1, gbm1, rf1, dl1) metalearner &lt;- &quot;h2o.glm.wrapper&quot; # stack existing models stack &lt;- h2o.stack( models = models, response_frame = train[,y], metalearner = metalearner, seed = 1, keep_levelone_data = TRUE ) # Compute test set performance: perf &lt;- h2o.ensemble_performance(stack, newdata = test) Print base learner and ensemble test set performance: print(perf) ## ## Base learner performance, sorted by specified metric: ## learner AUC ## 1 GLM_model_R_1522981883987_9864 0.6870611 ## 4 DeepLearning_model_R_1522981883987_11012 0.7248477 ## 3 DRF_model_R_1522981883987_10450 0.7691438 ## 2 GBM_model_R_1522981883987_9882 0.7817084 ## ## ## H2O Ensemble Performance on &lt;newdata&gt;: ## ---------------- ## Family: binomial ## ## Ensemble performance (AUC): 0.787627447904726 6.5.5.1 All done, shutdown H2O # good practice h2o.shutdown(prompt = FALSE) ## [1] TRUE "]
]
