<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>useR! Machine Learning Tutorial</title>
  <meta name="description" content="useR! 2016 Tutorial: Machine Learning Algorithmic Deep Dive.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="useR! Machine Learning Tutorial" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="useR! 2016 Tutorial: Machine Learning Algorithmic Deep Dive." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="useR! Machine Learning Tutorial" />
  
  <meta name="twitter:description" content="useR! 2016 Tutorial: Machine Learning Algorithmic Deep Dive." />
  

<meta name="author" content="Erin LeDell">


<meta name="date" content="2018-04-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="decision-trees.html">
<link rel="next" href="gradient-boosting-machines.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">UseR! Machine Learning Tutorial</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#overview"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dimensionality-issues"><i class="fa fa-check"></i>Dimensionality Issues</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sparsity"><i class="fa fa-check"></i>Sparsity</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#normalization"><i class="fa fa-check"></i>Normalization</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#categorical-data"><i class="fa fa-check"></i>Categorical Data</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#missing-data"><i class="fa fa-check"></i>Missing Data</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#class-imbalance"><i class="fa fa-check"></i>Class Imbalance</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#overfitting"><i class="fa fa-check"></i>Overfitting</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#scalability"><i class="fa fa-check"></i>Scalability</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#resources"><i class="fa fa-check"></i>Resources</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>1</b> Classification and Regression Trees (CART)</a><ul>
<li class="chapter" data-level="1.1" data-path="decision-trees.html"><a href="decision-trees.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="decision-trees.html"><a href="decision-trees.html#properties-of-trees"><i class="fa fa-check"></i><b>1.2</b> Properties of Trees</a></li>
<li class="chapter" data-level="1.3" data-path="decision-trees.html"><a href="decision-trees.html#tree-algorithms"><i class="fa fa-check"></i><b>1.3</b> Tree Algorithms</a></li>
<li class="chapter" data-level="1.4" data-path="decision-trees.html"><a href="decision-trees.html#cart-vs-c4.5"><i class="fa fa-check"></i><b>1.4</b> CART vs C4.5</a></li>
<li class="chapter" data-level="1.5" data-path="decision-trees.html"><a href="decision-trees.html#splitting-criterion-best-split"><i class="fa fa-check"></i><b>1.5</b> Splitting Criterion &amp; Best Split</a><ul>
<li class="chapter" data-level="1.5.1" data-path="decision-trees.html"><a href="decision-trees.html#gini-impurity"><i class="fa fa-check"></i><b>1.5.1</b> Gini Impurity</a></li>
<li class="chapter" data-level="1.5.2" data-path="decision-trees.html"><a href="decision-trees.html#entropy"><i class="fa fa-check"></i><b>1.5.2</b> Entropy</a></li>
<li class="chapter" data-level="1.5.3" data-path="decision-trees.html"><a href="decision-trees.html#information-gain"><i class="fa fa-check"></i><b>1.5.3</b> Information gain</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="decision-trees.html"><a href="decision-trees.html#decision-boundary"><i class="fa fa-check"></i><b>1.6</b> Decision Boundary</a></li>
<li class="chapter" data-level="1.7" data-path="decision-trees.html"><a href="decision-trees.html#missing-data-1"><i class="fa fa-check"></i><b>1.7</b> Missing Data</a></li>
<li class="chapter" data-level="1.8" data-path="decision-trees.html"><a href="decision-trees.html#visualizing-decision-trees"><i class="fa fa-check"></i><b>1.8</b> Visualizing Decision Trees</a></li>
<li class="chapter" data-level="1.9" data-path="decision-trees.html"><a href="decision-trees.html#cart-software-in-r"><i class="fa fa-check"></i><b>1.9</b> CART Software in R</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="random-forest.html"><a href="random-forest.html"><i class="fa fa-check"></i><b>2</b> Random Forests (RF)</a><ul>
<li class="chapter" data-level="2.1" data-path="random-forest.html"><a href="random-forest.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="random-forest.html"><a href="random-forest.html#history"><i class="fa fa-check"></i><b>2.2</b> History</a></li>
<li class="chapter" data-level="2.3" data-path="random-forest.html"><a href="random-forest.html#bagging"><i class="fa fa-check"></i><b>2.3</b> Bagging</a></li>
<li class="chapter" data-level="2.4" data-path="random-forest.html"><a href="random-forest.html#random-forest-algorithm"><i class="fa fa-check"></i><b>2.4</b> Random Forest Algorithm</a></li>
<li class="chapter" data-level="2.5" data-path="random-forest.html"><a href="random-forest.html#decision-boundary-1"><i class="fa fa-check"></i><b>2.5</b> Decision Boundary</a></li>
<li class="chapter" data-level="2.6" data-path="random-forest.html"><a href="random-forest.html#random-forest-by-randomization-aka-extra-trees"><i class="fa fa-check"></i><b>2.6</b> Random Forest by Randomization (aka “Extra-Trees”)</a></li>
<li class="chapter" data-level="2.7" data-path="random-forest.html"><a href="random-forest.html#out-of-bag-oob-estimates"><i class="fa fa-check"></i><b>2.7</b> Out-of-Bag (OOB) Estimates</a></li>
<li class="chapter" data-level="2.8" data-path="random-forest.html"><a href="random-forest.html#variable-importance"><i class="fa fa-check"></i><b>2.8</b> Variable Importance</a></li>
<li class="chapter" data-level="2.9" data-path="random-forest.html"><a href="random-forest.html#overfitting-1"><i class="fa fa-check"></i><b>2.9</b> Overfitting</a></li>
<li class="chapter" data-level="2.10" data-path="random-forest.html"><a href="random-forest.html#missing-data-2"><i class="fa fa-check"></i><b>2.10</b> Missing Data</a></li>
<li class="chapter" data-level="2.11" data-path="random-forest.html"><a href="random-forest.html#practical-uses"><i class="fa fa-check"></i><b>2.11</b> Practical Uses</a></li>
<li class="chapter" data-level="2.12" data-path="random-forest.html"><a href="random-forest.html#resources-1"><i class="fa fa-check"></i><b>2.12</b> Resources</a></li>
<li class="chapter" data-level="2.13" data-path="random-forest.html"><a href="random-forest.html#random-forest-software-in-r"><i class="fa fa-check"></i><b>2.13</b> Random Forest Software in R</a><ul>
<li class="chapter" data-level="2.13.1" data-path="random-forest.html"><a href="random-forest.html#randomforest"><i class="fa fa-check"></i><b>2.13.1</b> randomForest</a></li>
<li class="chapter" data-level="2.13.2" data-path="random-forest.html"><a href="random-forest.html#caret-method-parrf"><i class="fa fa-check"></i><b>2.13.2</b> caret method “parRF”</a></li>
<li class="chapter" data-level="2.13.3" data-path="random-forest.html"><a href="random-forest.html#h2o"><i class="fa fa-check"></i><b>2.13.3</b> h2o</a></li>
<li class="chapter" data-level="2.13.4" data-path="random-forest.html"><a href="random-forest.html#rborist"><i class="fa fa-check"></i><b>2.13.4</b> Rborist</a></li>
<li class="chapter" data-level="2.13.5" data-path="random-forest.html"><a href="random-forest.html#ranger"><i class="fa fa-check"></i><b>2.13.5</b> ranger</a></li>
</ul></li>
<li class="chapter" data-level="2.14" data-path="random-forest.html"><a href="random-forest.html#references"><i class="fa fa-check"></i><b>2.14</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html"><i class="fa fa-check"></i><b>3</b> Gradient Boosting Machines (GBM)</a><ul>
<li class="chapter" data-level="3.1" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#introduction-2"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#history-1"><i class="fa fa-check"></i><b>3.2</b> History</a></li>
<li class="chapter" data-level="3.3" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gradient-boosting"><i class="fa fa-check"></i><b>3.3</b> Gradient Boosting</a></li>
<li class="chapter" data-level="3.4" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#stagewise-additive-modeling"><i class="fa fa-check"></i><b>3.4</b> Stagewise Additive Modeling</a></li>
<li class="chapter" data-level="3.5" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#adaboost"><i class="fa fa-check"></i><b>3.5</b> AdaBoost</a></li>
<li class="chapter" data-level="3.6" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gradient-boosting-algorithm"><i class="fa fa-check"></i><b>3.6</b> Gradient Boosting Algorithm</a><ul>
<li class="chapter" data-level="3.6.1" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#loss-functions-and-gradients"><i class="fa fa-check"></i><b>3.6.1</b> Loss Functions and Gradients</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#stochastic-gbm"><i class="fa fa-check"></i><b>3.7</b> Stochastic GBM</a></li>
<li class="chapter" data-level="3.8" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#practical-tips"><i class="fa fa-check"></i><b>3.8</b> Practical Tips</a></li>
<li class="chapter" data-level="3.9" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#resources-2"><i class="fa fa-check"></i><b>3.9</b> Resources</a></li>
<li class="chapter" data-level="3.10" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm-software-in-r"><i class="fa fa-check"></i><b>3.10</b> GBM Software in R</a><ul>
<li class="chapter" data-level="3.10.1" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#gbm"><i class="fa fa-check"></i><b>3.10.1</b> gbm</a></li>
<li class="chapter" data-level="3.10.2" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#xgboost"><i class="fa fa-check"></i><b>3.10.2</b> xgboost</a></li>
<li class="chapter" data-level="3.10.3" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#h2o-1"><i class="fa fa-check"></i><b>3.10.3</b> h2o</a></li>
</ul></li>
<li class="chapter" data-level="3.11" data-path="gradient-boosting-machines.html"><a href="gradient-boosting-machines.html#references-1"><i class="fa fa-check"></i><b>3.11</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>4</b> Generalized Linear Models (GLM)</a><ul>
<li class="chapter" data-level="4.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#introduction-3"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#linear-models"><i class="fa fa-check"></i><b>4.2</b> Linear Models</a><ul>
<li class="chapter" data-level="4.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>4.2.1</b> Ordinary Least Squares (OLS)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#regularization"><i class="fa fa-check"></i><b>4.3</b> Regularization</a><ul>
<li class="chapter" data-level="4.3.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#ridge-regression"><i class="fa fa-check"></i><b>4.3.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="4.3.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#lasso-regression"><i class="fa fa-check"></i><b>4.3.2</b> Lasso Regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#elastic-net"><i class="fa fa-check"></i><b>4.3.3</b> Elastic Net</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#other-solvers"><i class="fa fa-check"></i><b>4.4</b> Other Solvers</a><ul>
<li class="chapter" data-level="4.4.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#iteratively-re-weighted-least-squares-irls"><i class="fa fa-check"></i><b>4.4.1</b> Iteratively Re-weighted Least Squares (IRLS)</a></li>
<li class="chapter" data-level="4.4.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#iteratively-re-weighted-least-squares-with-admm"><i class="fa fa-check"></i><b>4.4.2</b> Iteratively Re-weighted Least Squares with ADMM</a></li>
<li class="chapter" data-level="4.4.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#cyclical-coordinate-descent"><i class="fa fa-check"></i><b>4.4.3</b> Cyclical Coordinate Descent</a></li>
<li class="chapter" data-level="4.4.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#l-bfgs"><i class="fa fa-check"></i><b>4.4.4</b> L-BFGS</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#data-preprocessing"><i class="fa fa-check"></i><b>4.5</b> Data Preprocessing</a></li>
<li class="chapter" data-level="4.6" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#glm-software-in-r"><i class="fa fa-check"></i><b>4.6</b> GLM Software in R</a><ul>
<li class="chapter" data-level="4.6.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#glm"><i class="fa fa-check"></i><b>4.6.1</b> glm</a></li>
<li class="chapter" data-level="4.6.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#glm-in-caret"><i class="fa fa-check"></i><b>4.6.2</b> GLM in caret</a></li>
<li class="chapter" data-level="4.6.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#h2o-2"><i class="fa fa-check"></i><b>4.6.3</b> h2o</a></li>
<li class="chapter" data-level="4.6.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#speedglm"><i class="fa fa-check"></i><b>4.6.4</b> speedglm</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#regularized-glm-in-r"><i class="fa fa-check"></i><b>4.7</b> Regularized GLM in R</a><ul>
<li class="chapter" data-level="4.7.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#glmnet"><i class="fa fa-check"></i><b>4.7.1</b> glmnet</a></li>
<li class="chapter" data-level="4.7.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#h2o-3"><i class="fa fa-check"></i><b>4.7.2</b> h2o</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#references-2"><i class="fa fa-check"></i><b>4.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html"><i class="fa fa-check"></i><b>5</b> Deep Neural Networks (DNN)</a><ul>
<li class="chapter" data-level="5.1" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#introduction-4"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#history-2"><i class="fa fa-check"></i><b>5.2</b> History</a></li>
<li class="chapter" data-level="5.3" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>5.3</b> Backpropagation</a></li>
<li class="chapter" data-level="5.4" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#architectures"><i class="fa fa-check"></i><b>5.4</b> Architectures</a><ul>
<li class="chapter" data-level="5.4.1" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#multilayer-perceptron-mlp"><i class="fa fa-check"></i><b>5.4.1</b> Multilayer Perceptron (MLP)</a></li>
<li class="chapter" data-level="5.4.2" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#recurrent"><i class="fa fa-check"></i><b>5.4.2</b> Recurrent</a></li>
<li class="chapter" data-level="5.4.3" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#convolutional"><i class="fa fa-check"></i><b>5.4.3</b> Convolutional</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#visualizing-neural-nets"><i class="fa fa-check"></i><b>5.5</b> Visualizing Neural Nets</a></li>
<li class="chapter" data-level="5.6" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#deep-learning-software-in-r"><i class="fa fa-check"></i><b>5.6</b> Deep Learning Software in R</a><ul>
<li class="chapter" data-level="5.6.1" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#mxnet"><i class="fa fa-check"></i><b>5.6.1</b> MXNet</a></li>
<li class="chapter" data-level="5.6.2" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#h2o-4"><i class="fa fa-check"></i><b>5.6.2</b> h2o</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="deep-neural-networks.html"><a href="deep-neural-networks.html#references-3"><i class="fa fa-check"></i><b>5.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="stacking.html"><a href="stacking.html"><i class="fa fa-check"></i><b>6</b> Stacking</a><ul>
<li class="chapter" data-level="6.1" data-path="stacking.html"><a href="stacking.html#introduction-5"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="stacking.html"><a href="stacking.html#background"><i class="fa fa-check"></i><b>6.2</b> Background</a></li>
<li class="chapter" data-level="6.3" data-path="stacking.html"><a href="stacking.html#common-types-of-ensemble-methods"><i class="fa fa-check"></i><b>6.3</b> Common Types of Ensemble Methods</a><ul>
<li class="chapter" data-level="6.3.1" data-path="stacking.html"><a href="stacking.html#bagging-1"><i class="fa fa-check"></i><b>6.3.1</b> Bagging</a></li>
<li class="chapter" data-level="6.3.2" data-path="stacking.html"><a href="stacking.html#boosting"><i class="fa fa-check"></i><b>6.3.2</b> Boosting</a></li>
<li class="chapter" data-level="6.3.3" data-path="stacking.html"><a href="stacking.html#stacking"><i class="fa fa-check"></i><b>6.3.3</b> Stacking</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="stacking.html"><a href="stacking.html#the-super-learner-algorithm"><i class="fa fa-check"></i><b>6.4</b> The Super Learner Algorithm</a><ul>
<li class="chapter" data-level="6.4.1" data-path="stacking.html"><a href="stacking.html#set-up-the-ensemble"><i class="fa fa-check"></i><b>6.4.1</b> 1. Set up the ensemble</a></li>
<li class="chapter" data-level="6.4.2" data-path="stacking.html"><a href="stacking.html#train-the-ensemble"><i class="fa fa-check"></i><b>6.4.2</b> 2. Train the ensemble</a></li>
<li class="chapter" data-level="6.4.3" data-path="stacking.html"><a href="stacking.html#predict-on-new-data"><i class="fa fa-check"></i><b>6.4.3</b> 3. Predict on new data</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="stacking.html"><a href="stacking.html#stacking-software-in-r"><i class="fa fa-check"></i><b>6.5</b> Stacking Software in R</a><ul>
<li class="chapter" data-level="6.5.1" data-path="stacking.html"><a href="stacking.html#superlearner"><i class="fa fa-check"></i><b>6.5.1</b> SuperLearner</a></li>
<li class="chapter" data-level="6.5.2" data-path="stacking.html"><a href="stacking.html#subsemble"><i class="fa fa-check"></i><b>6.5.2</b> subsemble</a></li>
<li class="chapter" data-level="6.5.3" data-path="stacking.html"><a href="stacking.html#h2o-ensemble"><i class="fa fa-check"></i><b>6.5.3</b> H2O Ensemble</a></li>
<li class="chapter" data-level="6.5.4" data-path="stacking.html"><a href="stacking.html#higgs-demo"><i class="fa fa-check"></i><b>6.5.4</b> Higgs Demo</a></li>
<li class="chapter" data-level="6.5.5" data-path="stacking.html"><a href="stacking.html#stacking-existing-model-sets"><i class="fa fa-check"></i><b>6.5.5</b> Stacking Existing Model Sets</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">useR! Machine Learning Tutorial</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="random-forest" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Random Forests (RF)</h1>
<hr />
<div class="figure">
<img src="images/Forest3.jpg" title="Random Forest Drawing" />

</div>
<hr />
<p>Drawing by Phil Cutler.</p>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<p>Any tutorial on <a href="https://en.wikipedia.org/wiki/Random_forest">Random Forests</a> (RF) should also include a review of decicion trees, as these are models that are ensembled together to create the Random Forest model – or put another way, the “trees that comprise the forest.” Much of the complexity and detail of the Random Forest algorithm occurs within the individual decision trees and therefore it’s important to understand decision trees to understand the RF algorithm as a whole. Therefore, before proceeding, it is recommended that you read through the accompanying <a href="decision-trees.ipynb">Classification and Regression Trees Tutorial</a>.</p>
</div>
<div id="history" class="section level2">
<h2><span class="header-section-number">2.2</span> History</h2>
<p>The Random Forest algorithm is preceeded by the <a href="https://en.wikipedia.org/wiki/Random_subspace_method">Random SubspaceMethod</a> (aka “attribute bagging”), which accounts for half of the source of randomness in a Random Forest. The Random Subspace Method is an ensemble method that consists of several classifiers each operating in a subspace of the original feature space. The outputs of the models are then combined, usually by a simple majority vote. Tin Kam Ho applied the random subspace method to decision trees in 1995.</p>
<p><a href="https://en.wikipedia.org/wiki/Leo_Breiman">Leo Breiman</a> and <a href="http://www.math.usu.edu/~adele/">Adele Culter</a> combined Breiman’s <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">bagging</a> idea with the random subspace method to create a “Random Forest”, a name which is trademarked by the duo. Due to the trademark, the algorithm is sometimes called Random Decision Forests.</p>
<p>The introduction of random forests proper was first made in a <a href="http://www.stat.berkeley.edu/~breiman/randomforest2001.pdf">paper</a> by Leo Breiman [1]. This paper describes a method of building a forest of uncorrelated trees using a CART like procedure, combined with randomized node optimization and bagging. In addition, this paper combines several ingredients, some previously known and some novel, which form the basis of the modern practice of random forests, in particular:</p>
<ul>
<li>Using <a href="https://en.wikipedia.org/wiki/Out-of-bag_error">out-of-bag error</a> as an estimate of the <a href="https://en.wikipedia.org/wiki/Generalization_error">generalization error</a>.</li>
<li>Measuring <a href="https://en.wikipedia.org/wiki/Random_forest#Properties">variable importance</a> through permutation.</li>
</ul>
<p>The report also offers the first theoretical result for random forests in the form of a bound on the generalization error which depends on the strength of the trees in the forest and their correlation.</p>
<p>Although Brieman’s implemenation of Random Forests used his CART algorithm to construct the decision trees, many modern implementations of Random Forest use entropy-based algorithms for constructing the trees.</p>
</div>
<div id="bagging" class="section level2">
<h2><span class="header-section-number">2.3</span> Bagging</h2>
<p>Bagging (Bootstrap aggregating) was proposed by Leo Breiman in 1994 to improve the classification by combining classifications of randomly generated training sets. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the <a href="https://en.wikipedia.org/wiki/Ensemble_learning">model averaging</a> approach.</p>
<ul>
<li>Bagging or <em>bootstrap aggregation</em> averages a noisy fitted function, refit to many bootstrap samples to reduce it’s variance.</li>
<li>Bagging can dramatically reduce the variance of unstable procedures (like trees), leading to improved prediction, however any simple, interpretable, model structure (like that of a tree) is lost.</li>
<li>Bagging produces smoother decision boundaries than trees.</li>
</ul>
<p>The training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set <span class="math inline">\(X = x_1, ..., x_n\)</span> with responses <span class="math inline">\(Y = y_1, ..., y_n\)</span>, bagging repeatedly (<span class="math inline">\(B\)</span> times) selects a random sample with replacement of the training set and fits trees to these samples:</p>
<p>For <span class="math inline">\(b = 1, ..., B\)</span>: 1. Sample, with replacement, <span class="math inline">\(n\)</span> training examples from <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>; call these <span class="math inline">\(X_b\)</span>, <span class="math inline">\(Y_b\)</span>. 2. Train a decision or regression tree, <span class="math inline">\(f_b\)</span>, on <span class="math inline">\(X_b\)</span>, <span class="math inline">\(Y_b\)</span>.</p>
<p>After training, predictions for unseen samples <span class="math inline">\(x&#39;\)</span> can be made by averaging the predictions from all the individual regression trees on <span class="math inline">\(x&#39;\)</span>:</p>
<p><span class="math display">\[ {\hat {f}}={\frac {1}{B}}\sum _{b=1}^{B}{\hat {f}}_{b}(x&#39;)\]</span></p>
<p>or by taking the majority vote in the case of decision trees.</p>
</div>
<div id="random-forest-algorithm" class="section level2">
<h2><span class="header-section-number">2.4</span> Random Forest Algorithm</h2>
<p>The above procedure describes the original bagging algorithm for trees. <a href="https://en.wikipedia.org/wiki/Random_forest">Random Forests</a> differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called “feature bagging”.</p>
<ul>
<li>Random Forests correct for decision trees’ habit of overfitting to their training set.</li>
<li>Random Forest is an improvement over bagged trees that “de-correlates” the trees even further, reducing the variance.</li>
<li>At each tree split, a random sample of <span class="math inline">\(m\)</span> features is drawn and only those <span class="math inline">\(m\)</span> features are considered for splitting.</li>
<li>Typically <span class="math inline">\(m = \sqrt{p}\)</span> or <span class="math inline">\(\log_2p\)</span> where <span class="math inline">\(p\)</span> is the original number of features.</li>
<li>For each tree gown on a bootstrap sample, the error rates for observations left out of the bootstrap sample is monitored. This is called the <a href="https://en.wikipedia.org/wiki/Out-of-bag_error">“out-of- bag”</a> or OOB error rate.</li>
<li>Each tree has the same (statistical) <a href="https://en.wikipedia.org/wiki/Expected_value">expectation</a>, so increasing the number of trees does not alter the bias of bagging or the Random Forest algorithm.</li>
</ul>
</div>
<div id="decision-boundary-1" class="section level2">
<h2><span class="header-section-number">2.5</span> Decision Boundary</h2>
<p>This is an example of a decision boundary in two dimensions of a (binary) classification Random Forest. The black circle is the Bayes Optimal decision boundary and the blue square-ish boundary is learned by the classification tree.</p>
<p><img src="images/boundary_bagging.png" title="Bagging Bounday" /> Source: Elements of Statistical Learning</p>
</div>
<div id="random-forest-by-randomization-aka-extra-trees" class="section level2">
<h2><span class="header-section-number">2.6</span> Random Forest by Randomization (aka “Extra-Trees”)</h2>
<p>In <a href="http://link.springer.com/article/10.1007%2Fs10994-006-6226-1">Extremely Randomized Trees</a> (aka Extra- Trees) [2], randomness goes one step further in the way splits are computed. As in Random Forests, a random subset of candidate features is used, but instead of looking for the best split, thresholds (for the split) are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias.</p>
<p>Extremely Randomized Trees is implemented in the <a href="https://cran.r-project.org/web/packages/extraTrees/index.html">extraTrees</a> R package and also available in the <a href="https://0xdata.atlassian.net/browse/PUBDEV-2837">h2o</a> R package as part of the <code>h2o.randomForest()</code> function via the <code>histogram_type = &quot;Random&quot;</code> argument.</p>
</div>
<div id="out-of-bag-oob-estimates" class="section level2">
<h2><span class="header-section-number">2.7</span> Out-of-Bag (OOB) Estimates</h2>
<p>In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows:</p>
<ul>
<li>Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree.</li>
<li>Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees.</li>
<li>At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n averaged over all cases is the oob error estimate. This has proven to be unbiased in many tests.</li>
</ul>
</div>
<div id="variable-importance" class="section level2">
<h2><span class="header-section-number">2.8</span> Variable Importance</h2>
<p>In every tree grown in the forest, put down the OOB cases and count the number of votes cast for the correct class. Now randomly permute the values of variable m in the oob cases and put these cases down the tree. Subtract the number of votes for the correct class in the variable-<span class="math inline">\(m\)</span>-permuted OOB data from the number of votes for the correct class in the untouched OOB data. The average of this number over all trees in the forest is the raw importance score for variable <span class="math inline">\(m\)</span>.</p>
<p>If the values of this score from tree to tree are independent, then the standard error can be computed by a standard computation. The correlations of these scores between trees have been computed for a number of data sets and proved to be quite low, therefore we compute standard errors in the classical way, divide the raw score by its standard error to get a <span class="math inline">\(z\)</span>-score, ands assign a significance level to the <span class="math inline">\(z\)</span>-score assuming normality.</p>
<p>If the number of variables is very large, forests can be run once with all the variables, then run again using only the most important variables from the first run.</p>
<p>For each case, consider all the trees for which it is oob. Subtract the percentage of votes for the correct class in the variable-<span class="math inline">\(m\)</span>-permuted OOB data from the percentage of votes for the correct class in the untouched OOB data. This is the local importance score for variable m for this case.</p>
<p>Variable importance in Extremely Randomized Trees is explained <a href="http://www.slideshare.net/glouppe/understanding-variable-importances-in-%20forests-of-randomized-trees">here</a>.</p>
</div>
<div id="overfitting-1" class="section level2">
<h2><span class="header-section-number">2.9</span> Overfitting</h2>
<p>Leo Brieman famously claimed that “Random Forests do not overfit.” This is perhaps not exactly the case, however they are certainly more robust to overfitting than a Gradient Boosting Machine (GBM). Random Forests can be overfit by growing trees that are “too deep”, for example. However, it is hard to overfit a Random Forest by adding more trees to the forest – typically that will increase accuracy (at the expense of computation time).</p>
</div>
<div id="missing-data-2" class="section level2">
<h2><span class="header-section-number">2.10</span> Missing Data</h2>
<p>Missing values do not neccessarily have to be imputed in a Random Forest implemenation, although some software packages will require it.</p>
</div>
<div id="practical-uses" class="section level2">
<h2><span class="header-section-number">2.11</span> Practical Uses</h2>
<p>Here is a short article called, <a href="https://medium.com/rants-on-machine-learning/the-unreasonable-%20effectiveness-of-random-forests-f33c3ce28883#.r734znc9f">The Unreasonable Effectiveness of Random Forests</a>, by Ahmed El Deeb, about the utility of Random Forests. It summarizes some of the algorithm’s pros and cons nicely.</p>
</div>
<div id="resources-1" class="section level2">
<h2><span class="header-section-number">2.12</span> Resources</h2>
<ul>
<li><a href="http://arxiv.org/abs/1407.7502">Gilles Louppe - Understanding Random Forests (PhD Dissertation)</a> (pdf)</li>
<li><a href="http://www.slideshare.net/glouppe/understanding-random-forests-from-%20theory-to-practice">Gilles Louppe - Understanding Random Forests: From Theory to Practice</a> (slides)</li>
<li><a href="https:/%20/www.youtube.com/watch?v=wPqtzj5VZus&amp;index=16&amp;list=PLNtMya54qvOFQhSZ4IKKXRbMkyLM%20n0caa">Trevor Hastie - Gradient Boosting &amp; Random Forests at H2O World 2014</a> (YouTube)</li>
<li><a href="https://www.youtube.com/watch?v=9wn1f-30_ZY">Mark Landry - Gradient Boosting Method and Random Forest at H2O World 2015</a> (YouTube)</li>
</ul>
<hr />
</div>
<div id="random-forest-software-in-r" class="section level2">
<h2><span class="header-section-number">2.13</span> Random Forest Software in R</h2>
<p>The oldest and most well known implementation of the Random Forest algorithm in R is the <a href="https://cran.r-project.org/web/packages/randomForest/index.html">randomForest</a> package. There are also a number of packages that implement variants of the algorithm, and in the past few years, there have been several “big data” focused implementations contributed to the R ecosystem as well.</p>
<p>Here is a non-comprehensive list:</p>
<ul>
<li><a href="http://www.rdocumentation.org/packages/randomFore%20st/functions/randomForest">randomForest::randomForest</a></li>
<li><a href="http://www.rdocumentation.org/packages/h2o/functions/h%202o.randomForest">h2o::h2o.randomForest</a></li>
<li><a href="https://github.com/vertica/DistributedR/b%20lob/master/algorithms/HPdclassifier/R/hpdRF_parallelForest.R">DistributedR::hpdRF_parallelForest</a></li>
<li><a href="http://www.rdocumentation.org/packages/party/functions/cfores%20t">party::cForest</a>: A random forest variant for response variables measured at arbitrary scales based on conditional inference trees.</li>
<li><a href="https://cran.r-project.org/web/packages/randomForestSRC/inde%20x.html">randomForestSRC</a> implements a unified treatment of Breiman’s random forests for survival, regression and classification problems.</li>
<li><a href="https://cran.r-project.org/web/packages/quantregForest/index.%20html">quantregForest</a> can regress quantiles of a numeric response on exploratory variables via a random forest approach.</li>
<li><a href="https://cran.r-project.org/web/packages/ranger/index.html">ranger</a></li>
<li><a href="https://cran.r-project.org/web/packages/Rborist/index.html">Rborist</a></li>
<li>The <a href="https://topepo.github.io/caret/index.html">caret</a> package wraps a number of different Random Forest packages in R (<a href="https://topepo.github.io/caret/Random_Forest.html">full list here</a>):</li>
<li>Conditional Inference Random Forest (<code>party::cForest</code>)</li>
<li>Oblique Random Forest (<code>obliqueRF</code>)</li>
<li>Parallel Random Forest (<code>randomForest</code> + <code>foreach</code>)</li>
<li>Random Ferns (<code>rFerns</code>)</li>
<li>Random Forest (<code>randomForest</code>)</li>
<li>Random Forest (<code>ranger</code>)</li>
<li>Quantile Random Forest (<code>quantregForest</code>)</li>
<li>Random Forest by Randomization (<code>extraTrees</code>)</li>
<li>Random Forest Rule-Based Model (<code>inTrees</code>)</li>
<li>Random Forest with Additional Feature Selection (<code>Boruta</code>)</li>
<li>Regularized Random Forest (<code>RRF</code>)</li>
<li>Rotation Forest (<code>rotationForest</code>)</li>
<li>Weighted Subspace Random Forest (<code>wsrf</code>)</li>
<li>The <a href="https://github.com/mlr-org/mlr">mlr</a> package wraps a number of different Random Forest packages in R:</li>
<li>Conditional Inference Random Forest (<code>party::cForest</code>)</li>
<li>Rotation Forest (<code>rotationForest</code>)</li>
<li>Parallel Forest (<code>ParallelForest</code>)</li>
<li>Survival Forest (<code>randomForestSRC</code>)</li>
<li>Random Ferns (<code>rFerns</code>)</li>
<li>Random Forest (<code>randomForest</code>)</li>
<li>Random Forest (<code>ranger</code>)</li>
<li>Synthetic Random Forest (<code>randomForestSRC</code>)</li>
<li>Random Uniform Forest (<code>randomUniformForest</code>)</li>
</ul>
<p>Since there are so many different Random Forest implementations available, there have been several benchmarks to compare the performance of popular implementations, including implementations outside of R. A few examples:</p>
<ol style="list-style-type: decimal">
<li><a href="http://www.wise.io/tech/benchmarking-random-forest-part-1">Benchmarking Random Forest Classification</a> by Erin LeDell, 2013</li>
<li><a href="http://datascience.la/benchmarking-random-forest-implementations/">Benchmarking Random Forest Implementations</a> by Szilard Pafka, 2015</li>
<li><a href="http://arxiv.org/pdf/1508.04409v1.pdf">Ranger</a> publication by Marvin N. Wright and Andreas Ziegler, 2015</li>
</ol>
<div id="randomforest" class="section level3">
<h3><span class="header-section-number">2.13.1</span> randomForest</h3>
<p>Authors: Fortran original by <a href="http://www.stat.berkeley.edu/~breiman/">LeoBreiman</a> and <a href="http://www.math.usu.edu/~adele/">Adele Cutler</a>, R port by <a href="https://www.linkedin.com/in/andy-liaw-1399347">AndyLiaw</a> and Matthew Wiener.</p>
<p>Backend: Fortran</p>
<p>Features:</p>
<ul>
<li>This package wraps the original Fortran code by Leo Breiman and Adele Culter and is probably the most widely known/used implemenation in R.</li>
<li>Single-threaded.</li>
<li>Although it’s single-threaded, smaller forests can be trained in parallel by writing custom <a href="https://cran.r-project.org/web/packages/foreach/index.html">foreach</a> or <a href="http://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf">parall el</a> code, then combined into a bigger forest using the <a href="htt%20p://www.rdocumentation.org/packages/randomForest/functions/combine">randomForest::combine()</a> function.</li>
<li>Row weights unimplemented (been on the wishlist for as long as I can remember).</li>
<li>Uses CART trees split by Gini Impurity.</li>
<li>Categorical predictors are allowed to have up to 53 categories.</li>
<li>Multinomial response can have no more than 32 categories.</li>
<li>Supports R formula interface (but I’ve read some reports that claim it’s slower when the formula interface is used).</li>
<li>GPL-2/3 Licensed.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># randomForest example</span>
<span class="co"># install.packages(&quot;randomForest&quot;)</span>
<span class="co"># install.packages(&quot;cvAUC&quot;)</span>
<span class="kw">library</span>(randomForest)
<span class="kw">library</span>(cvAUC)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load binary-response dataset</span>
train &lt;-<span class="st"> </span>data.table<span class="op">::</span><span class="kw">fread</span>(<span class="st">&quot;https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv&quot;</span>)
test &lt;-<span class="st"> </span>data.table<span class="op">::</span><span class="kw">fread</span>(<span class="st">&quot;https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv&quot;</span>)

<span class="co"># convert to regular data frames</span>
train &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(train)
test &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(test)

<span class="co"># Dimensions</span>
<span class="kw">dim</span>(train)
## [1] 10000    29
<span class="kw">dim</span>(test)
## [1] 5000   29

<span class="co"># Columns</span>
<span class="kw">names</span>(train)
##  [1] &quot;response&quot; &quot;x1&quot;       &quot;x2&quot;       &quot;x3&quot;       &quot;x4&quot;       &quot;x5&quot;      
##  [7] &quot;x6&quot;       &quot;x7&quot;       &quot;x8&quot;       &quot;x9&quot;       &quot;x10&quot;      &quot;x11&quot;     
## [13] &quot;x12&quot;      &quot;x13&quot;      &quot;x14&quot;      &quot;x15&quot;      &quot;x16&quot;      &quot;x17&quot;     
## [19] &quot;x18&quot;      &quot;x19&quot;      &quot;x20&quot;      &quot;x21&quot;      &quot;x22&quot;      &quot;x23&quot;     
## [25] &quot;x24&quot;      &quot;x25&quot;      &quot;x26&quot;      &quot;x27&quot;      &quot;x28&quot;</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Identity the response column</span>
ycol &lt;-<span class="st"> &quot;response&quot;</span>

<span class="co"># Identify the predictor columns</span>
xcols &lt;-<span class="st"> </span><span class="kw">setdiff</span>(<span class="kw">names</span>(train), ycol)

<span class="co"># Convert response to factor (required by randomForest)</span>
train[,ycol] &lt;-<span class="st"> </span><span class="kw">as.factor</span>(train[,ycol])
test[,ycol] &lt;-<span class="st"> </span><span class="kw">as.factor</span>(test[,ycol])</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train a default RF model with 500 trees</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)  <span class="co"># For reproducibility</span>
<span class="kw">system.time</span>(
  model &lt;-<span class="st"> </span><span class="kw">randomForest</span>(
    <span class="dt">x =</span> train[,xcols], 
    <span class="dt">y =</span> train[,ycol],
    <span class="dt">xtest =</span> test[,xcols],
    <span class="dt">ntree =</span> <span class="dv">500</span>
    )
  )
##    user  system elapsed 
##  18.460   0.052  18.566</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Generate predictions on test dataset</span>
preds &lt;-<span class="st"> </span>model<span class="op">$</span>test<span class="op">$</span>votes[, <span class="dv">2</span>]
labels &lt;-<span class="st"> </span>test[,ycol]

<span class="co"># Compute AUC on the test set</span>
cvAUC<span class="op">::</span><span class="kw">AUC</span>(<span class="dt">predictions =</span> preds, <span class="dt">labels =</span> labels)
## [1] 0.7834743</code></pre></div>
</div>
<div id="caret-method-parrf" class="section level3">
<h3><span class="header-section-number">2.13.2</span> caret method “parRF”</h3>
<p>Authors: Max Kuhn</p>
<p>Backend: Fortran (wraps the <code>randomForest</code> package)</p>
<p>This is a wrapper for the <code>randomForest</code> package that parallelizes the tree building.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
<span class="kw">library</span>(doParallel)
<span class="kw">library</span>(e1071)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set up parallel environment</span>
cl &lt;-<span class="st"> </span><span class="kw">makeCluster</span>(<span class="dv">4</span>) 
<span class="kw">registerDoParallel</span>(cl) 

<span class="co"># Train a &quot;parRF&quot; model using caret</span>
model &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(
  <span class="dt">x =</span> train[,xcols],
  <span class="dt">y =</span> train[,ycol],
  <span class="dt">method =</span> <span class="st">&quot;parRF&quot;</span>,
  <span class="dt">preProcess =</span> <span class="ot">NULL</span>,
  <span class="dt">weights =</span> <span class="ot">NULL</span>,
  <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,
  <span class="dt">maximize =</span> <span class="ot">TRUE</span>,
  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(),
  <span class="dt">tuneGrid =</span> <span class="ot">NULL</span>,
  <span class="dt">tuneLength =</span> <span class="dv">3</span>
  )

<span class="co"># good practice to shut down parallel environment</span>
<span class="kw">stopCluster</span>(cl) </code></pre></div>
</div>
<div id="h2o" class="section level3">
<h3><span class="header-section-number">2.13.3</span> h2o</h3>
<p>Authors: <a href="http://www.cs.purdue.edu/homes/jv/">Jan Vitek</a>, <a href="https://www.linkedin.com/in/candel">Arno Candel</a>, H2O.ai contributors</p>
<p>Backend: Java</p>
<p>Features:</p>
<ul>
<li>Distributed and parallelized computation on either a single node or a multi- node cluster.</li>
<li>Automatic early stopping based on convergence of user-specied metrics to user- specied relative tolerance.</li>
<li>Data-distributed, which means the entire dataset does not need to fit into memory on a single node.</li>
<li>Uses histogram approximations of continuous variables for speedup.</li>
<li>Uses squared error to determine optimal splits.</li>
<li>Automatic early stopping based on convergence of user-specied metrics to user- specied relative tolerance.</li>
<li>Support for exponential families (Poisson, Gamma, Tweedie) and loss functions in addition to binomial (Bernoulli), Gaussian and multinomial distributions, such as Quantile regression (including Laplace).</li>
<li>Grid search for hyperparameter optimization and model selection.</li>
<li>Apache 2.0 Licensed.</li>
<li>Model export in plain Java code for deployment in production environments.</li>
<li>GUI for training &amp; model eval/viz (H2O Flow).</li>
</ul>
<p>Implementation details are presented in slidedecks by <a href="http://www.slideshare.net/0xdata/rf-brighttalk">Michal Mahalova</a> and <a href="http://www.slideshare.net/0xdata/jan-vitek-%20distributedrandomforest522013">Jan Vitek</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#install.packages(&quot;h2o&quot;)</span>
<span class="kw">library</span>(h2o)
<span class="co">#h2o.shutdown(prompt = FALSE)</span>
<span class="kw">h2o.init</span>(<span class="dt">nthreads =</span> <span class="op">-</span><span class="dv">1</span>)  <span class="co">#Start a local H2O cluster using nthreads = num available cores</span>
## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmpf8Tb8n/h2o_bradboehmke_started_from_r.out
##     /var/folders/ws/qs4y2bnx1xs_4y9t0zbdjsvh0000gn/T//Rtmpf8Tb8n/h2o_bradboehmke_started_from_r.err
## 
## 
## Starting H2O JVM and connecting: ... Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         3 seconds 197 milliseconds 
##     H2O cluster timezone:       America/New_York 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.18.0.4 
##     H2O cluster version age:    28 days, 3 hours and 9 minutes  
##     H2O cluster name:           H2O_started_from_R_bradboehmke_gpo922 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   1.78 GB 
##     H2O cluster total cores:    4 
##     H2O cluster allowed cores:  4 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.4.4 (2018-03-15)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># convert data to h2o objects</span>
train &lt;-<span class="st"> </span><span class="kw">as.h2o</span>(train)
## 
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|</span><span class="st">                                                                 </span><span class="er">|</span><span class="st">   </span><span class="dv">0</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|=================================================================|</span><span class="st"> </span><span class="dv">100</span>%
test &lt;-<span class="st"> </span><span class="kw">as.h2o</span>(test)
## 
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|</span><span class="st">                                                                 </span><span class="er">|</span><span class="st">   </span><span class="dv">0</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|=================================================================|</span><span class="st"> </span><span class="dv">100</span>%

<span class="co"># Dimensions</span>
<span class="kw">dim</span>(train)
## [1] 10000    29
<span class="kw">dim</span>(test)
## [1] 5000   29

<span class="co"># Columns</span>
<span class="kw">names</span>(train)
##  [1] &quot;response&quot; &quot;x1&quot;       &quot;x2&quot;       &quot;x3&quot;       &quot;x4&quot;       &quot;x5&quot;      
##  [7] &quot;x6&quot;       &quot;x7&quot;       &quot;x8&quot;       &quot;x9&quot;       &quot;x10&quot;      &quot;x11&quot;     
## [13] &quot;x12&quot;      &quot;x13&quot;      &quot;x14&quot;      &quot;x15&quot;      &quot;x16&quot;      &quot;x17&quot;     
## [19] &quot;x18&quot;      &quot;x19&quot;      &quot;x20&quot;      &quot;x21&quot;      &quot;x22&quot;      &quot;x23&quot;     
## [25] &quot;x24&quot;      &quot;x25&quot;      &quot;x26&quot;      &quot;x27&quot;      &quot;x28&quot;</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Identity the response column</span>
ycol &lt;-<span class="st"> &quot;response&quot;</span>

<span class="co"># Identify the predictor columns</span>
xcols &lt;-<span class="st"> </span><span class="kw">setdiff</span>(<span class="kw">names</span>(train), ycol)

<span class="co"># Convert response to factor (required by randomForest)</span>
train[,ycol] &lt;-<span class="st"> </span><span class="kw">as.factor</span>(train[,ycol])
test[,ycol] &lt;-<span class="st"> </span><span class="kw">as.factor</span>(test[,ycol])</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train a default RF model with 500 trees</span>
<span class="kw">system.time</span>(
  model &lt;-<span class="st"> </span><span class="kw">h2o.randomForest</span>(
    <span class="dt">x =</span> xcols,
    <span class="dt">y =</span> ycol,
    <span class="dt">training_frame =</span> train,
    <span class="dt">seed =</span> <span class="dv">1</span>, <span class="co">#for reproducibility</span>
    <span class="dt">ntrees =</span> <span class="dv">500</span>
    )
  ) 
## 
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|</span><span class="st">                                                                 </span><span class="er">|</span><span class="st">   </span><span class="dv">0</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|=</span><span class="st">                                                                </span><span class="er">|</span><span class="st">   </span><span class="dv">2</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|==</span><span class="st">                                                               </span><span class="er">|</span><span class="st">   </span><span class="dv">3</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|===</span><span class="st">                                                              </span><span class="er">|</span><span class="st">   </span><span class="dv">5</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|====</span><span class="st">                                                             </span><span class="er">|</span><span class="st">   </span><span class="dv">7</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|=====</span><span class="st">                                                            </span><span class="er">|</span><span class="st">   </span><span class="dv">8</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|========</span><span class="st">                                                         </span><span class="er">|</span><span class="st">  </span><span class="dv">13</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|============</span><span class="st">                                                     </span><span class="er">|</span><span class="st">  </span><span class="dv">18</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|===============</span><span class="st">                                                  </span><span class="er">|</span><span class="st">  </span><span class="dv">23</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|===================</span><span class="st">                                              </span><span class="er">|</span><span class="st">  </span><span class="dv">29</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|======================</span><span class="st">                                           </span><span class="er">|</span><span class="st">  </span><span class="dv">35</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|==========================</span><span class="st">                                       </span><span class="er">|</span><span class="st">  </span><span class="dv">40</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|==============================</span><span class="st">                                   </span><span class="er">|</span><span class="st">  </span><span class="dv">45</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|=================================</span><span class="st">                                </span><span class="er">|</span><span class="st">  </span><span class="dv">51</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|====================================</span><span class="st">                             </span><span class="er">|</span><span class="st">  </span><span class="dv">56</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|========================================</span><span class="st">                         </span><span class="er">|</span><span class="st">  </span><span class="dv">62</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|===========================================</span><span class="st">                      </span><span class="er">|</span><span class="st">  </span><span class="dv">67</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|===============================================</span><span class="st">                  </span><span class="er">|</span><span class="st">  </span><span class="dv">73</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|===================================================</span><span class="st">              </span><span class="er">|</span><span class="st">  </span><span class="dv">78</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|======================================================</span><span class="st">           </span><span class="er">|</span><span class="st">  </span><span class="dv">84</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|==========================================================</span><span class="st">       </span><span class="er">|</span><span class="st">  </span><span class="dv">90</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|==============================================================</span><span class="st">   </span><span class="er">|</span><span class="st">  </span><span class="dv">95</span>%
  <span class="op">|</span><span class="st">                                                                       </span>
<span class="st">  </span><span class="er">|=================================================================|</span><span class="st"> </span><span class="dv">100</span>%
##    user  system elapsed 
##   0.570   0.027  25.513</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Compute AUC on test dataset</span>
<span class="co"># H2O computes many model performance metrics automatically, including AUC</span>

perf &lt;-<span class="st"> </span><span class="kw">h2o.performance</span>(<span class="dt">model =</span> model, <span class="dt">newdata =</span> test)
<span class="kw">h2o.auc</span>(perf)
## [1] 0.785394</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># good practice to shut down h2o environment</span>
<span class="kw">h2o.shutdown</span>(<span class="dt">prompt =</span> <span class="ot">FALSE</span>)
## [1] TRUE</code></pre></div>
</div>
<div id="rborist" class="section level3">
<h3><span class="header-section-number">2.13.4</span> Rborist</h3>
<p>Authors: Mark Seligman</p>
<p>Backend: C++</p>
<p>The <a href="https://github.com/suiji/Arborist">Arborist</a> provides a fast, open-source implementation of the Random Forest algorithm. The Arborist achieves its speed through efficient C++ code and parallel, distributed tree construction. This <a href="http://www.rinfinance.com/agenda/2015/talk/MarkSeligman.pdf">slidedeck</a> provides detail about the implementation and vision of the project.</p>
<p>Features:</p>
<ul>
<li>Began as proprietary implementation, but was open-sourced and rewritten following dissolution of venture.</li>
<li>Project called “Aborist” but R package is called “Rborist”. A Python interface is in development.</li>
<li>CPU based but a GPU version called Curborist (Cuda Rborist) is in development (unclear if it will be open source).</li>
<li>Unlimited factor cardinality.</li>
<li>Emphasizes multi-core but not multi-node.</li>
<li>Both Python support and GPU support have been “coming soon” since summer 2015, not sure the status of the projects.</li>
<li>GPL-2/3 licensed.</li>
</ul>
</div>
<div id="ranger" class="section level3">
<h3><span class="header-section-number">2.13.5</span> ranger</h3>
<p>Authors: <a href="http://www.imbs-luebeck.de/imbs/node/323">Marvin N. Wright</a> and Andreas Ziegler</p>
<p>Backend: C++</p>
<p><a href="http://arxiv.org/pdf/1508.04409v1.pdf">Ranger</a> is a fast <a href="https://github.com/imbs-hl/ranger">implementation</a> of random forest (Breiman 2001) or recursive partitioning, particularly suited for high dimensional data. Classification, regression, probability estimation and survival forests are supported. Classification and regression forests are implemented as in the original Random Forest (Breiman 2001), survival forests as in Random Survival Forests (Ishwaran et al. 2008). For probability estimation forests see Malley et al. (2012).</p>
<p>Features:</p>
<ul>
<li>Multi-threaded.</li>
<li>Direct support for <a href="https://en.wikipedia.org/wiki/Genome-%20wide_association_study">GWAS</a> (Genome-wide association study) data.</li>
<li>Excellent speed and support for high-dimensional or wide data.</li>
<li>Not as fast for “tall &amp; skinny” data (many rows, few columns).</li>
<li>GPL-3 licensed.</li>
</ul>
<p><img src="images/ranger_vs_arborist.png" title="Ranger vs Rborist" alt="Alt text" /> Plot from the <a href="http://arxiv.org/pdf/1508.04409v1.pdf">ranger article</a>.</p>
</div>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">2.14</span> References</h2>
<p>[1] [<a href="http://www.stat.berkeley.edu/~breiman/randomforest2001.pdf" class="uri">http://www.stat.berkeley.edu/~breiman/randomforest2001.pdf</a>](<a href="http://www.stat.berkeley.edu/~breiman/randomforest2001.pdf" class="uri">http://www.stat.berkeley.edu/~breiman/randomforest2001.pdf</a>)</p>
<p>[2] [P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,Machine Learning, 63(1), 3-42,2006.](<a href="http://link.springer.com/article/10.1007%2Fs10994-006-6226-1">http://link.springer.com/article/10.1007%2Fs10994-006-6226-1</a>)</p>
<p>[3] [<a href="http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf" class="uri">http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf</a>](<a href="http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf" class="uri">http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf</a>)</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="decision-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="gradient-boosting-machines.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-random-forest.Rmd",
"text": "Edit"
},
"download": ["bookdown-start.pdf", "bookdown-start.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
